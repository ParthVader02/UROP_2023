{
 "cells": [
  {
   "cell_type": "raw",
   "id": "24b61755",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd318a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "\n",
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785e9ccd",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7136089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for showing images\n",
    "def imshow(img):\n",
    "    #img = img / 2 + 0.5  \n",
    "    #print(img)\n",
    "    #plt.imshow can take in images with 0-1 (floats) or 0-255 (int)\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0))) \n",
    "    #print(img)\n",
    "    #print(max(img[0][0]))\n",
    "    print(np.mean(img[0][2]))\n",
    "    print(np.std(img[0][2]))\n",
    "    #print(max(img[0][1]))\n",
    "    #print(max(img[0][2]))\n",
    "#It's best to keep the data processing code separate from the class loader\n",
    "def to_tensor_and_normalize(imagepil): #Done with testing\n",
    "    \"\"\"Convert image to torch Tensor and normalize using the ImageNet training\n",
    "    set mean and stdev taken from\n",
    "    https://pytorch.org/docs/stable/torchvision/models.html.\n",
    "    Why the ImageNet mean and stdev instead of the PASCAL VOC mean and stdev?\n",
    "    Because we are using a model pretrained on ImageNet.\"\"\"\n",
    "    #Think the reason for introducing normalisation is because of the imagenet weights\n",
    "    #ChosenTransforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "    #            torchvision.transforms.Normalize(mean=[0.6236, 0.5118, 0.4264],std=[0.3545, 0.2692, 0.3376]),])\n",
    "\n",
    "\n",
    "    #This straight up just transforms 0-255 to 0-1\n",
    "    ChosenTransforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])#, torchvision.transforms.Normalize(mean=[0.3, 0.3, 0.3],std=[0.05, 0.05, 0.05])])\n",
    "    #ChosenTransforms = torchvision.transforms.Compose([torchvision.transforms.PILToTensor()])#[torchvision.transforms.ToTensor()])\n",
    "    \"\"\"ChosenTransforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                                      lambda x: x*1.666666666666666666667,\n",
    "                                                      ])\"\"\"\n",
    "    return ChosenTransforms(imagepil)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e00af3d",
   "metadata": {},
   "source": [
    "# Class for loading in my simulated image training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79d16792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimData(Dataset):\n",
    "    def __init__(self, setname):\n",
    "        '''The sim input images and output images are 128 x 128 x 3'''\n",
    "        self.setname = setname\n",
    "        assert setname in ['train','val','test']\n",
    "        \n",
    "        #Define where to load in the dataset\n",
    "        overall_dataset_dir = os.path.join(os.path.join(os.getcwd(),'load_dataset'), 'data')\n",
    "        #input images\n",
    "        self.selected_dataset_dir = os.path.join(overall_dataset_dir,setname)\n",
    "        \n",
    "        #output images\n",
    "        self.selected_dataset_output_dir = os.path.join(overall_dataset_dir,setname+\"_outputs\")\n",
    "\n",
    "        count = 0\n",
    "        # Iterate directory\n",
    "        for path in os.listdir(self.selected_dataset_dir): #get number of images in folder\n",
    "            # check if current path is a file\n",
    "            if os.path.isfile(os.path.join(self.selected_dataset_dir, path)):\n",
    "                count += 1 # increment count\n",
    "        \n",
    "        #E.g. self.all_filenames = ['im1.jpg',..,'imN.jpg'] when setname=='train'\n",
    "        #Loads in the input images from the training folder\n",
    "        self.all_filenames = [] #initalise list of filenames\n",
    "        self.all_filenames_output = [] \n",
    "        for i in range(1,count+1):  #iterate through all the images by number rather than os.listdir which does it randomly\n",
    "            self.all_filenames.append(\"im\"+str(i)+\".jpg\")\n",
    "            self.all_filenames_output.append(\"im\"+str(i)+\".jpg\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of examples in this split, e.g. if\n",
    "        self.setname=='train' then return the total number of examples\n",
    "        in the training set\"\"\"\n",
    "        return len(self.all_filenames)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \"\"\"Return the example at index [idx]. The example is a dict with keys\n",
    "        'data' (value: Tensor for an RGB image) and labels are also images\"\"\"\n",
    "        #For the inputs\n",
    "        selected_filename = self.all_filenames[idx]\n",
    "        #print(selected_filename)\n",
    "        #test= self.all_filenames_output[idx]\n",
    "        #print(test)\n",
    "        imagepil = PIL.Image.open(os.path.join(self.selected_dataset_dir,selected_filename)).convert('RGB')\n",
    "        \n",
    "        #For the outputs\n",
    "        selected_filename_output = self.all_filenames_output[idx]\n",
    "        imagepil_output = PIL.Image.open(os.path.join(self.selected_dataset_output_dir,\n",
    "                                                      selected_filename_output)).convert('RGB')\n",
    "        \n",
    "        #convert image to Tensor/normalize\n",
    "        image = to_tensor_and_normalize(imagepil)\n",
    "        image_output = to_tensor_and_normalize(imagepil_output)\n",
    "        \n",
    "        \n",
    "        sample = {'data':image, #preprocessed image, for input into NN\n",
    "                  'label':image_output,\n",
    "                  'img_idx':idx}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754edf6c",
   "metadata": {},
   "source": [
    "# Now to load in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73076b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Basically load the created class in\n",
    "# train_dataset = SimData(\"train\")\n",
    "\n",
    "# #Prepare data loaders\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, num_workers=0, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0068bdf",
   "metadata": {},
   "source": [
    "# Display the loaded in images for a sanity check lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2447c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataiter = iter(train_loader)\n",
    "# #For some reason this is needed\n",
    "# output = next(dataiter)\n",
    "# images = output['data'].numpy()\n",
    "# labels = output['label'].numpy()\n",
    "# #print(labels)\n",
    "# #Rather than this\n",
    "# #images, labels, _ = next(dataiter)\n",
    "# #images = images.numpy() # convert images to numpy for display\n",
    "\n",
    "\n",
    "# #Plot the images\n",
    "# print(\"Input Images (Sensor Data)\")\n",
    "# fig, axes = plt.subplots(nrows=1, ncols=5, sharex=True, sharey=True, figsize=(12,4))\n",
    "# for idx in np.arange(5):\n",
    "#     ax = fig.add_subplot(1, 5, idx+1, xticks=[], yticks=[])\n",
    "#     imshow(images[idx])\n",
    "#     #ax.set_title(classes[labels[idx]])\n",
    "# plt.show()\n",
    "\n",
    "# #Output images\n",
    "# print(\"Shape output Images\")\n",
    "# fig, axes = plt.subplots(nrows=1, ncols=5, sharex=True, sharey=True, figsize=(12,4))\n",
    "# for idx in np.arange(5):\n",
    "#     ax = fig.add_subplot(1, 5, idx+1, xticks=[], yticks=[])\n",
    "#     imshow(labels[idx])\n",
    "#     #ax.set_title(classes[labels[idx]])\n",
    "# plt.show() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e6a8486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(images[0].shape)\n",
    "# print(labels[0].shape)\n",
    "# print(images[0])\n",
    "# print(np.max(images[0][1]))\n",
    "# print(np.mean(images[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96914298",
   "metadata": {},
   "source": [
    "#  Conv autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec92c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, layer_disp = False):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        self.layer_disp = layer_disp\n",
    "        #Encoder\n",
    "        #nn.Conv2(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, \n",
    "                               out_channels=128,\n",
    "                               kernel_size=2, \n",
    "                               stride = 1,\n",
    "                               padding = 'same')\n",
    "        self.conv2 = nn.Conv2d(in_channels=128, \n",
    "                               out_channels=256,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding = 0)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=256, \n",
    "                               out_channels=256,\n",
    "                               kernel_size=4, \n",
    "                               stride = 4,\n",
    "                               padding = 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        \n",
    "       \n",
    "        #Decoder\n",
    "        #nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.t_conv1 = nn.ConvTranspose2d(in_channels=256, \n",
    "                               out_channels=128,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)\n",
    "        self.t_conv2= nn.ConvTranspose2d(in_channels=128, \n",
    "                               out_channels=128,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)\n",
    "        self.t_conv3= nn.ConvTranspose2d(in_channels=128, \n",
    "                               out_channels=64,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)\n",
    "        \n",
    "        self.t_conv4= nn.ConvTranspose2d(in_channels=64, \n",
    "                               out_channels=64,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)\n",
    "        self.t_conv5= nn.ConvTranspose2d(in_channels=64, \n",
    "                               out_channels=32,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)\n",
    "        '''self.t_conv6= nn.ConvTranspose2d(in_channels=32, \n",
    "                               out_channels=32,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)'''\n",
    "       \n",
    "        self.t_convout = nn.ConvTranspose2d(in_channels=32, \n",
    "                               out_channels=3,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)\n",
    "        #self.upsample = nn.functional.interpolate(scale_factor = 2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        en_layer1 = F.relu(self.conv1(x))   \n",
    "        en_layer1 = self.pool(en_layer1)\n",
    "        en_layer2 = F.relu(self.conv2(en_layer1))\n",
    "        en_layer2 = self.pool(en_layer2)\n",
    "        en_layer3 = F.relu(self.conv3(en_layer2))\n",
    "        en_layer3 = self.pool(en_layer3)\n",
    "\n",
    "        #de_layer0 = F.relu(self.t_conv0(en_layer4)) \n",
    "        de_layer1 = F.relu(self.t_conv1(en_layer3))   \n",
    "        #de_layer1 = nn.functional.interpolate(de_layer1,scale_factor = 2)\n",
    "        de_layer2= F.relu(self.t_conv2(de_layer1)) \n",
    "        de_layer3= F.relu(self.t_conv3(de_layer2))\n",
    "        de_layer4= F.relu(self.t_conv4(de_layer3))\n",
    "        de_layer5= F.relu(self.t_conv5(de_layer4))\n",
    "        #de_layer6= F.relu(self.t_conv6(de_layer5))\n",
    "        \n",
    "        #de_layerout = torch.sigmoid(self.t_conv3(de_layer2))\n",
    "        de_layerout = F.relu(self.t_convout(de_layer5))\n",
    "        #de_layerout = self.t_conv5(de_layer4)\n",
    "        \n",
    "        if self.layer_disp:\n",
    "            print(\"input\", x.shape)\n",
    "            print(\"en_layer1\",en_layer1.shape)\n",
    "            #print(\"en_layer1a\",en_layer1a.shape)\n",
    "            print(\"en_layer2\",en_layer2.shape)\n",
    "            #print(\"en_layer2a\",en_layer2a.shape)\n",
    "            print(\"en_layer3\",en_layer3.shape)\n",
    "            #print(\"en_layer3a\",en_layer3a.shape)\n",
    "            #print(\"en_layer4\",en_layer4.shape)\n",
    "            #print(\"en_layer5\",en_layer5.shape)\n",
    "            \n",
    "            #print(\"de_layer0\",de_layer0.shape)\n",
    "            print(\"de_layer1\",de_layer1.shape)\n",
    "            #print(\"de_layer1a\",de_layer1a.shape)\n",
    "            print(\"de_layer2\",de_layer2.shape)\n",
    "            print(\"de_layer3\",de_layer3.shape)\n",
    "            print(\"de_layer4\",de_layer4.shape)\n",
    "            print(\"de_layer5\",de_layer5.shape)\n",
    "            #print(\"de_layer6\",de_layer6.shape)\n",
    "            #print(\"de_layer2a\",de_layer2a.shape)\n",
    "            #print(\"de_layer3\",de_layerou.shape)\n",
    "            #print(\"de_layer3a\",de_layer3a.shape)\n",
    "            #print(\"de_layer4\",de_layer4.shape)\n",
    "            print(\"de_layerout\",de_layerout.shape)\n",
    "              \n",
    "        return de_layerout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "179b2f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([3, 128, 128])\n",
      "en_layer1 torch.Size([128, 64, 64])\n",
      "en_layer2 torch.Size([256, 16, 16])\n",
      "en_layer3 torch.Size([256, 2, 2])\n",
      "de_layer1 torch.Size([128, 4, 4])\n",
      "de_layer2 torch.Size([128, 8, 8])\n",
      "de_layer3 torch.Size([64, 16, 16])\n",
      "de_layer4 torch.Size([64, 32, 32])\n",
      "de_layer5 torch.Size([32, 64, 64])\n",
      "de_layerout torch.Size([3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "#Now to intialise the model \n",
    "model = ConvAutoencoder(layer_disp = False).to(device)\n",
    "model = ConvAutoencoder(layer_disp = True).to(device)\n",
    "\n",
    "#Defining the loss function between the input and the output\n",
    "#criterion = nn.BCELoss()\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.L1Loss()\n",
    "#Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1*1e-3)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 50, gamma=0.5)\n",
    "\n",
    "#This is just for testing and seeing the outputs of each convolutional layer\n",
    "dummy = model(torch.empty(3, 128, 128).to(device))\n",
    "#from torchviz import make_dot\n",
    "#make_dot(dummy, params=dict(list(model.named_parameters()))).render(\"autoencoder\", format=\"png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc309965",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa9d813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from livelossplot import PlotLosses\n",
    "\n",
    "# n_epochs = 10000 #Number of epochs\n",
    "\n",
    "# liveloss = PlotLosses()\n",
    "# logs = {}\n",
    "\n",
    "\n",
    "# for epoch in range(1, n_epochs+1):\n",
    "#     # monitor training loss\n",
    "#     train_loss = 0.0\n",
    "\n",
    "#     #Training\n",
    "#     for data in train_loader:\n",
    "#         #For some reason this is needed\n",
    "#         output = data\n",
    "#         #print(output)\n",
    "#         images = output['data']\n",
    "#         labels = output['label']\n",
    "#         #images, _ = data #Don't care about the labels\n",
    "#         images = images.to(device) \n",
    "#         labels = labels.to(device) \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(images)\n",
    "#         #outputs = model(labels) #to test if the autoencoder works\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_loss += loss.item()*images.size(0)\n",
    "          \n",
    "#     train_loss = train_loss/len(train_loader)\n",
    "#     logs['loss'] = train_loss\n",
    "    \n",
    "#     #print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n",
    "#     predictions = outputs\n",
    "#     predictions = predictions.detach().cpu().numpy()\n",
    "#     imshow(predictions[0])\n",
    "#     if not(epoch%50) or epoch ==0:\n",
    "#         plt.show()\n",
    "\n",
    "#     liveloss.update(logs)\n",
    "#     liveloss.send()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db8086d",
   "metadata": {},
   "source": [
    "# Display the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ea27f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([39, 3, 128, 128])\n",
      "en_layer1 torch.Size([39, 128, 64, 64])\n",
      "en_layer2 torch.Size([39, 256, 16, 16])\n",
      "en_layer3 torch.Size([39, 256, 2, 2])\n",
      "de_layer1 torch.Size([39, 128, 4, 4])\n",
      "de_layer2 torch.Size([39, 128, 8, 8])\n",
      "de_layer3 torch.Size([39, 64, 16, 16])\n",
      "de_layer4 torch.Size([39, 64, 32, 32])\n",
      "de_layer5 torch.Size([39, 32, 64, 64])\n",
      "de_layerout torch.Size([39, 3, 128, 128])\n",
      "(39, 3, 128, 128)\n",
      "torch.Size([39, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# #Basically load the created class in\n",
    "# test_dataset = SimData(\"test\")\n",
    "\n",
    "# model.load_state_dict(torch.load(\"/home/parth/UROP_2023/deblur.pth\", map_location=torch.device('cpu')))\n",
    "# model.eval()\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=40, num_workers=0, shuffle = False)\n",
    "\n",
    "# #Batch of test images\n",
    "# with torch.no_grad():\n",
    "#     dataiter = iter(test_loader)\n",
    "#     #images, labels = next(dataiter)\n",
    "#     output = next(dataiter)\n",
    "#     images = output['data'].to(device) #these are the inputs\n",
    "#     labels = output['label'].to(device)\n",
    "#     #labels = output['label'].numpy() #again labels aren't needed\n",
    "\n",
    "#     #Sample outputs\n",
    "#     #predictions = model(labels) #test just the atoencoder ibit\n",
    "#     predictions = model(images) #output of the network\n",
    "#     images = images.detach().cpu().numpy() #convert the images back to numpy arrays\n",
    "#     labels = labels.detach().cpu().numpy() #convert the images back to numpy arrays\n",
    "\n",
    "#     #convert back to appropriate numpy array and shit\n",
    "#     print(images.shape)\n",
    "#     print(predictions.shape)\n",
    "#     predictions = predictions.detach().cpu().numpy()\n",
    "#     #predictions = predictions.view(batch_size, 3, 32, 32)\n",
    "\n",
    "#     for idx in np.arange(len(test_dataset)): \n",
    "#         #save input images\n",
    "#         input = np.transpose(images[idx], (1, 2, 0)) #convert to correct format\n",
    "#         input = np.array(255*input, dtype = 'uint8') \n",
    "#         input = cv2.cvtColor(input, cv2.COLOR_RGB2BGR) \n",
    "#         os.makedirs('inputs', exist_ok=True) \n",
    "#         base_path = os.path.join('inputs',\"im{}.jpg\".format(idx+1)) \n",
    "#         cv2.imwrite(base_path,input)\n",
    "\n",
    "#         #save output images\n",
    "#         pred = np.transpose(predictions[idx], (1, 2, 0)) #convert to correct format\n",
    "#         pred = np.array(255*pred, dtype = 'uint8')\n",
    "#         pred = cv2.cvtColor(pred, cv2.COLOR_RGB2BGR)\n",
    "#         os.makedirs('predictions', exist_ok=True) \n",
    "#         base_path = os.path.join('predictions',\"im{}.jpg\".format(idx+1))\n",
    "#         cv2.imwrite(base_path,pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68b443b",
   "metadata": {},
   "source": [
    "# Testing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8565695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "{'predictions': [{'x': 42.0, 'y': 51.5, 'width': 42.0, 'height': 53.0, 'confidence': 0.5040665864944458, 'class': 'o', 'image_path': '/home/parth/UROP_2023/autoencoder/predictions/im1.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 41.5, 'y': 70.0, 'width': 41.0, 'height': 50.0, 'confidence': 0.4367355704307556, 'class': 'i', 'image_path': '/home/parth/UROP_2023/autoencoder/predictions/im1.jpg', 'prediction_type': 'ObjectDetectionModel'}], 'image': {'width': '128', 'height': '128'}}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 177\u001b[0m\n\u001b[1;32m    174\u001b[0m os\u001b[39m.\u001b[39mmakedirs(\u001b[39m'\u001b[39m\u001b[39m/home/parth/UROP_2023/autoencoder/braille_detect\u001b[39m\u001b[39m'\u001b[39m, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m#make folder to store braille classifier images\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(test_dataset)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m): \u001b[39m#for each image in the predictions folder\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     \u001b[39m#infer on each image\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     pred_dict \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39;49mpredict(\u001b[39m\"\u001b[39;49m\u001b[39m/home/parth/UROP_2023/autoencoder/predictions/im\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m.jpg\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mformat(i), confidence\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, overlap\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m)\u001b[39m.\u001b[39mjson() \u001b[39m#save as dictionary data type\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[39mprint\u001b[39m(pred_dict)\n\u001b[1;32m    179\u001b[0m     \u001b[39m#save classified image to braille_detect folder\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/roboflow/models/object_detection.py:187\u001b[0m, in \u001b[0;36mObjectDetectionModel.predict\u001b[0;34m(self, image_path, hosted, format, classes, overlap, confidence, stroke, labels)\u001b[0m\n\u001b[1;32m    185\u001b[0m     img_str \u001b[39m=\u001b[39m img_str\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    186\u001b[0m     \u001b[39m# Post to API and return response\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     resp \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mpost(\n\u001b[1;32m    188\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi_url,\n\u001b[1;32m    189\u001b[0m         data\u001b[39m=\u001b[39;49mimg_str,\n\u001b[1;32m    190\u001b[0m         headers\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mContent-Type\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mapplication/x-www-form-urlencoded\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[1;32m    191\u001b[0m     )\n\u001b[1;32m    193\u001b[0m     image_dims \u001b[39m=\u001b[39m {\n\u001b[1;32m    194\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39m(original_dimensions[\u001b[39m0\u001b[39m]),\n\u001b[1;32m    195\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mheight\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39m(original_dimensions[\u001b[39m1\u001b[39m]),\n\u001b[1;32m    196\u001b[0m     }\n\u001b[1;32m    197\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(image_path, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m    198\u001b[0m     \u001b[39m# Performing inference on a OpenCV2 frame\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(url, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, json\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, data\u001b[39m=\u001b[39;49mdata, json\u001b[39m=\u001b[39;49mjson, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1376\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##############################################################################################################\n",
    "#import libraries\n",
    "\n",
    "import cv2\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "from threading import Thread\n",
    "import re\n",
    "import json\n",
    "from roboflow import Roboflow\n",
    "\n",
    "sys.path.insert(0, '/home/parth/UROP_2023/Generic_ur5_controller') #add path to brailley controller\n",
    "from digit_demo import DigitSensor, DisplayImage\n",
    "import kg_robot as kgr\n",
    "##############################################################################################################\n",
    "#get target text file properties\n",
    "\n",
    "props = []\n",
    "with open('test_properties.csv', 'r') as f:\n",
    "    props = f.read()\n",
    "print(props)\n",
    "word_count = int(props[0])\n",
    "letter_count = int(props[1])\n",
    "line_count = int(props[2])\n",
    "##############################################################################################################\n",
    "#read target text file on braille reader\n",
    "\n",
    "total_rows = line_count  #found from target text file\n",
    "dynamic_count = 1\n",
    "row_counter = 1\n",
    "\n",
    "z_depth = 0.0143 #set z depth of sensor, with medical tape need to be lower for clarity\n",
    "y_offset = -0.27 #set y offset of sensor\n",
    "\n",
    "velocity = 0.2 #initialise velocity\n",
    "start = 0\n",
    "end = 0\n",
    "slide_capture_flag = False #initialise slide capture flag\n",
    "\n",
    "with (DigitSensor(serialno='D20654', resolution='QVGA', framerate='60') as digit,\n",
    "            DisplayImage(window_name='DIGIT Demo') as display): #use wrapper to make accessing DIGIT sensor easier\n",
    "    frame = digit.get_frame()\n",
    "    print(\"------------Configuring brailley------------\\r\\n\")\n",
    "    brailley = kgr.kg_robot(port=30000,db_host=\"169.254.252.50\")\n",
    "    print(\"----------------Hi brailley!----------------\\r\\n\\r\\n\")\n",
    "\n",
    "    def read_camera(): #function to read camera\n",
    "         global frame\n",
    "         global start\n",
    "         while True:\n",
    "            frame = digit.get_frame() #get frame from camera\n",
    "            if slide_capture_flag == True: \n",
    "                capture_frame(\"test_blurry\") #capture frame\n",
    "\n",
    "    def capture_frame(dir_path): #function to capture frame and save it\n",
    "        global static_collect, dynamic_collect, static_count, dynamic_count\n",
    "        os.makedirs(dir_path, exist_ok=True)  \n",
    "        base_path = os.path.join(dir_path,\"im{}.jpg\".format(dynamic_count)) #create path to save frame\n",
    "        cv2.imwrite(base_path, frame)\n",
    "        dynamic_count += 1 #increment counts\n",
    "    \n",
    "    def move_robot(): #fixed movements for each data collection step\n",
    "        global dynamic_count, velocity, row_counter\n",
    "        global slide_capture_flag\n",
    "        global start\n",
    "\n",
    "        slide_capture_flag = True\n",
    "        brailley.movel([0.296, y_offset, z_depth,  2.21745, 2.22263, -0.00201733], 500, velocity) #slide across one row\n",
    "        slide_capture_flag = False\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        row_counter += 1 #increment row counter\n",
    "        scroll_button() #press scroll button\n",
    "\n",
    "    def scroll_button():\n",
    "        brailley.movel([0.305801, -0.261322, 0.0186874, 2.21758, 2.22249, -0.00198903], 0.5, 0.2) #move to scroll position\n",
    "        time.sleep(0.5)\n",
    "        brailley.translatel_rel([0, 0, -0.006, 0, 0, 0], 0.5, 0.2) #press scroll button\n",
    "        time.sleep(0.5)\n",
    "        brailley.translatel_rel([0, 0, 0.006, 0, 0, 0], 0.5, 0.2) #move back to scroll position\n",
    "        time.sleep(0.5)\n",
    "        brailley.movel([0.296,y_offset, z_depth+0.01, 2.21745, 2.22263, -0.00201733], 0.5, 0.2) #move above first position\n",
    "        time.sleep(0.5)\n",
    "        brailley.movel([0.296,y_offset, z_depth, 2.21745, 2.22263, -0.00201733], 0.5, 0.2) #move to first position\n",
    "\n",
    "    if __name__=='__main__':\n",
    "        t= Thread(target=read_camera) #start thread to read camera\n",
    "        t.daemon = True #set thread to daemon so it closes when main thread closes\n",
    "        t.start()\n",
    "        \n",
    "        brailley.movel([0.169, y_offset, z_depth+0.01, 2.21745, 2.22263, -0.00101733], 0.5, 0.2) #move above first position\n",
    "        time.sleep(0.5)\n",
    "        brailley.movel([0.169, y_offset, z_depth, 2.21745, 2.22263, -0.00201733], 0.5, 0.2) #move to first position\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        print(\"------------Starting data collection------------\\r\\n\")\n",
    "        start = time.time() #start timer\n",
    "        while row_counter <= total_rows: #get at least the target data set size\n",
    "            move_robot() #movements\n",
    "            print(\"Row {} of {} collected\".format(row_counter, total_rows)) #print progress\n",
    "        end = time.time()\n",
    "        time_taken = end-start\n",
    "        wpm_speed = word_count/(time_taken/60) #calculate words per minute\n",
    "        print(\"Time taken: {} seconds\".format(time_taken)) #print time taken\n",
    "        print(\"Words: {}\".format(word_count))\n",
    "        print(\"Speed: {} words per minute\".format(wpm_speed))\n",
    "print(\"------------Data collection complete------------\\r\\n\") \n",
    "##############################################################################################################\n",
    "#process images for autoencoder\n",
    "\n",
    "os.rmdir('/home/parth/UROP_2023/autoencoder/load_dataset/data/test') #remove any pre-existing test folder\n",
    "os.rmdir('/home/parth/UROP_2023/autoencoder/load_dataset/data/test_outputs') #remove any pre-existing test_outputs folder\n",
    "\n",
    "for image in os.scandir('test_blurry'): #for each image in the blurry folder\n",
    "    path = image.path\n",
    "    num = (re.findall(r'\\d+', path))[0] #get image number\n",
    "\n",
    "    img = cv2.imread(path) #read image\n",
    "    new_size = (128, 128) # new_size=(width, height)\n",
    "    img = cv2.resize(img, new_size) \n",
    "\n",
    "    os.makedirs('/home/parth/UROP_2023/autoencoder/load_dataset/data/test', exist_ok=True) #save resized images to test folder\n",
    "    base_path = os.path.join('/home/parth/UROP_2023/autoencoder/load_dataset/data/test',\"im{}.jpg\".format(num)) \n",
    "    cv2.imwrite(base_path, img)\n",
    "\n",
    "    os.makedirs('/home/parth/UROP_2023/autoencoder/load_dataset/data/test_outputs', exist_ok=True) #need to create test_outputs folder to work with SimData class\n",
    "    base_path = os.path.join('/home/parth/UROP_2023/autoencoder/load_dataset/data/test_outputs',\"im{}.jpg\".format(num)) \n",
    "    cv2.imwrite(base_path, img)\n",
    "##############################################################################################################\n",
    "#load blurry images into autoencoder\n",
    "\n",
    "test_dataset = SimData(\"test\") #load test dataset as SimData class\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, num_workers=0, shuffle = False) #no shuffle so images go in order\n",
    "\n",
    "model.load_state_dict(torch.load(\"/home/parth/UROP_2023/deblur.pth\", map_location=torch.device('cpu'))) #load pre-trained autoencdoer model\n",
    "model.eval() #set model to evaluation mode\n",
    "\n",
    "#input blurry images into autoencoder\n",
    "with torch.no_grad():\n",
    "    dataiter = iter(test_loader)\n",
    "    output = next(dataiter)\n",
    "    images = output['data'].to(device) #these are the inputs\n",
    "    labels = output['label'].to(device)\n",
    "\n",
    "    #Sample outputs\n",
    "    predictions = model(images) #output of the network\n",
    "    images = images.detach().cpu().numpy() #convert the images back to numpy arrays\n",
    "    labels = labels.detach().cpu().numpy() #convert the images back to numpy arrays\n",
    "\n",
    "    #convert back to appropriate numpy array \n",
    "    print(images.shape)\n",
    "    print(predictions.shape)\n",
    "    predictions = predictions.detach().cpu().numpy()\n",
    "\n",
    "    for idx in np.arange(len(test_dataset)): \n",
    "        #save input images\n",
    "        input = np.transpose(images[idx], (1, 2, 0)) #convert to correct format\n",
    "        input = np.array(255*input, dtype = 'uint8') \n",
    "        input = cv2.cvtColor(input, cv2.COLOR_RGB2BGR) \n",
    "        os.makedirs('inputs', exist_ok=True) \n",
    "        base_path = os.path.join('inputs',\"im{}.jpg\".format(idx+1)) \n",
    "        cv2.imwrite(base_path,input)\n",
    "\n",
    "        #save output images\n",
    "        pred = np.transpose(predictions[idx], (1, 2, 0)) #convert to correct format\n",
    "        pred = np.array(255*pred, dtype = 'uint8')\n",
    "        pred = cv2.cvtColor(pred, cv2.COLOR_RGB2BGR)\n",
    "        os.makedirs('predictions', exist_ok=True) \n",
    "        base_path = os.path.join('predictions',\"im{}.jpg\".format(idx+1))\n",
    "        cv2.imwrite(base_path,pred) \n",
    "\n",
    "##############################################################################################################\n",
    "#input deblurred images into pre-trained YOLO v8 model\n",
    "\n",
    "rf = Roboflow(api_key=\"qnNaV5QQtZevwcJxsA5Y\") #create roboflow object\n",
    "project = rf.workspace().project(\"digit-braille\") \n",
    "classifier = project.version(3).model #load pre-trained model\n",
    "pred_text = \"\" #initialise string of predicted text\n",
    "\n",
    "os.makedirs('/home/parth/UROP_2023/autoencoder/braille_detect', exist_ok=True) #make folder to store braille classifier images\n",
    "for i in range(1, len(test_dataset)+1): #for each image in the predictions folder\n",
    "    #infer on each image\n",
    "    pred_json = classifier.predict(\"/home/parth/UROP_2023/autoencoder/predictions/im{}.jpg\".format(i), confidence=50, overlap=30).json() #save as dictionary data type\n",
    "    pred_dict = json.loads(pred_json) #convert to dictionary\n",
    "    if 'predictions' in pred_dict:\n",
    "        preds = pred_dict['predictions'] #get predictions from dictionary\n",
    "        for pred in preds:\n",
    "            pred_text += pred['class'] #add predicted character to string\n",
    "    else:\n",
    "        pred_text += ' ' #add empty space if no predictions\n",
    "    #save bounding box image to braille_detect folder\n",
    "    classifier.predict(\"/home/parth/UROP_2023/autoencoder/predictions/im{}.jpg\".format(i), confidence=50, overlap=30).save(\"/home/parth/UROP_2023/autoencoder/braille_detect/detect{}.jpg\".format(i))\n",
    "##############################################################################################################\n",
    "#Compare predicted text with target text\n",
    "\n",
    "pred_text = pred_text[::-1] #reverse predicted text as we are reading from right to left (to reduce effect of rolling shutter)\n",
    "print(pred_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
