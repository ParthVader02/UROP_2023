{
 "cells": [
  {
   "cell_type": "raw",
   "id": "24b61755",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd318a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "\n",
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785e9ccd",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7136089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for showing images\n",
    "def imshow(img):\n",
    "    #img = img / 2 + 0.5  \n",
    "    #print(img)\n",
    "    #plt.imshow can take in images with 0-1 (floats) or 0-255 (int)\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0))) \n",
    "    #print(img)\n",
    "    #print(max(img[0][0]))\n",
    "    print(np.mean(img[0][2]))\n",
    "    print(np.std(img[0][2]))\n",
    "    #print(max(img[0][1]))\n",
    "    #print(max(img[0][2]))\n",
    "#It's best to keep the data processing code separate from the class loader\n",
    "def to_tensor_and_normalize(imagepil): #Done with testing\n",
    "    \"\"\"Convert image to torch Tensor and normalize using the ImageNet training\n",
    "    set mean and stdev taken from\n",
    "    https://pytorch.org/docs/stable/torchvision/models.html.\n",
    "    Why the ImageNet mean and stdev instead of the PASCAL VOC mean and stdev?\n",
    "    Because we are using a model pretrained on ImageNet.\"\"\"\n",
    "    #Think the reason for introducing normalisation is because of the imagenet weights\n",
    "    #ChosenTransforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "    #            torchvision.transforms.Normalize(mean=[0.6236, 0.5118, 0.4264],std=[0.3545, 0.2692, 0.3376]),])\n",
    "\n",
    "\n",
    "    #This straight up just transforms 0-255 to 0-1\n",
    "    ChosenTransforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])#, torchvision.transforms.Normalize(mean=[0.3, 0.3, 0.3],std=[0.05, 0.05, 0.05])])\n",
    "    #ChosenTransforms = torchvision.transforms.Compose([torchvision.transforms.PILToTensor()])#[torchvision.transforms.ToTensor()])\n",
    "    \"\"\"ChosenTransforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                                      lambda x: x*1.666666666666666666667,\n",
    "                                                      ])\"\"\"\n",
    "    return ChosenTransforms(imagepil)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e00af3d",
   "metadata": {},
   "source": [
    "# Class for loading in my simulated image training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79d16792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimData(Dataset):\n",
    "    def __init__(self, setname):\n",
    "        '''The sim input images and output images are 128 x 128 x 3'''\n",
    "        self.setname = setname\n",
    "        assert setname in ['train','val','test']\n",
    "        \n",
    "        #Define where to load in the dataset\n",
    "        overall_dataset_dir = os.path.join(os.path.join(os.getcwd(),'load_dataset'), 'data')\n",
    "        #input images\n",
    "        self.selected_dataset_dir = os.path.join(overall_dataset_dir,setname)\n",
    "        \n",
    "        #output images\n",
    "        self.selected_dataset_output_dir = os.path.join(overall_dataset_dir,setname+\"_outputs\")\n",
    "        \n",
    "        #E.g. self.all_filenames = ['im1.jpg',..,'imN.jpg'] when setname=='train'\n",
    "        #Loads in the input images from the training folder\n",
    "        self.all_filenames = os.listdir(self.selected_dataset_dir)\n",
    "        self.all_filenames_output = os.listdir(self.selected_dataset_output_dir)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of examples in this split, e.g. if\n",
    "        self.setname=='train' then return the total number of examples\n",
    "        in the training set\"\"\"\n",
    "        return len(self.all_filenames)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \"\"\"Return the example at index [idx]. The example is a dict with keys\n",
    "        'data' (value: Tensor for an RGB image) and labels are also images\"\"\"\n",
    "        #For the inputs\n",
    "        selected_filename = self.all_filenames[idx]\n",
    "        #print(selected_filename)\n",
    "        #test= self.all_filenames_output[idx]\n",
    "        #print(test)\n",
    "        imagepil = PIL.Image.open(os.path.join(self.selected_dataset_dir,selected_filename)).convert('RGB')\n",
    "        \n",
    "        #For the outputs\n",
    "        selected_filename_output = self.all_filenames_output[idx]\n",
    "        imagepil_output = PIL.Image.open(os.path.join(self.selected_dataset_output_dir,\n",
    "                                                      selected_filename_output)).convert('RGB')\n",
    "        \n",
    "        #convert image to Tensor/normalize\n",
    "        image = to_tensor_and_normalize(imagepil)\n",
    "        image_output = to_tensor_and_normalize(imagepil_output)\n",
    "        \n",
    "        \n",
    "        sample = {'data':image, #preprocessed image, for input into NN\n",
    "                  'label':image_output,\n",
    "                  'img_idx':idx}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754edf6c",
   "metadata": {},
   "source": [
    "# Now to load in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73076b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Basically load the created class in\n",
    "# train_dataset = SimData(\"train\")\n",
    "\n",
    "# #Prepare data loaders\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, num_workers=0, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0068bdf",
   "metadata": {},
   "source": [
    "# Display the loaded in images for a sanity check lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2447c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataiter = iter(train_loader)\n",
    "# #For some reason this is needed\n",
    "# output = next(dataiter)\n",
    "# images = output['data'].numpy()\n",
    "# labels = output['label'].numpy()\n",
    "# #print(labels)\n",
    "# #Rather than this\n",
    "# #images, labels, _ = next(dataiter)\n",
    "# #images = images.numpy() # convert images to numpy for display\n",
    "\n",
    "\n",
    "# #Plot the images\n",
    "# print(\"Input Images (Sensor Data)\")\n",
    "# fig, axes = plt.subplots(nrows=1, ncols=5, sharex=True, sharey=True, figsize=(12,4))\n",
    "# for idx in np.arange(5):\n",
    "#     ax = fig.add_subplot(1, 5, idx+1, xticks=[], yticks=[])\n",
    "#     imshow(images[idx])\n",
    "#     #ax.set_title(classes[labels[idx]])\n",
    "# plt.show()\n",
    "\n",
    "# #Output images\n",
    "# print(\"Shape output Images\")\n",
    "# fig, axes = plt.subplots(nrows=1, ncols=5, sharex=True, sharey=True, figsize=(12,4))\n",
    "# for idx in np.arange(5):\n",
    "#     ax = fig.add_subplot(1, 5, idx+1, xticks=[], yticks=[])\n",
    "#     imshow(labels[idx])\n",
    "#     #ax.set_title(classes[labels[idx]])\n",
    "# plt.show() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e6a8486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(images[0].shape)\n",
    "# print(labels[0].shape)\n",
    "# print(images[0])\n",
    "# print(np.max(images[0][1]))\n",
    "# print(np.mean(images[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96914298",
   "metadata": {},
   "source": [
    "#  Conv autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec92c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, layer_disp = False):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        self.layer_disp = layer_disp\n",
    "        #Encoder\n",
    "        #nn.Conv2(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, \n",
    "                               out_channels=128,\n",
    "                               kernel_size=2, \n",
    "                               stride = 1,\n",
    "                               padding = 'same')\n",
    "        self.conv2 = nn.Conv2d(in_channels=128, \n",
    "                               out_channels=256,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding = 0)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=256, \n",
    "                               out_channels=256,\n",
    "                               kernel_size=4, \n",
    "                               stride = 4,\n",
    "                               padding = 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        \n",
    "       \n",
    "        #Decoder\n",
    "        #nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.t_conv1 = nn.ConvTranspose2d(in_channels=256, \n",
    "                               out_channels=128,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)\n",
    "        self.t_conv2= nn.ConvTranspose2d(in_channels=128, \n",
    "                               out_channels=128,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)\n",
    "        self.t_conv3= nn.ConvTranspose2d(in_channels=128, \n",
    "                               out_channels=64,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)\n",
    "        \n",
    "        self.t_conv4= nn.ConvTranspose2d(in_channels=64, \n",
    "                               out_channels=64,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)\n",
    "        self.t_conv5= nn.ConvTranspose2d(in_channels=64, \n",
    "                               out_channels=32,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)\n",
    "        '''self.t_conv6= nn.ConvTranspose2d(in_channels=32, \n",
    "                               out_channels=32,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)'''\n",
    "       \n",
    "        self.t_convout = nn.ConvTranspose2d(in_channels=32, \n",
    "                               out_channels=3,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)\n",
    "        #self.upsample = nn.functional.interpolate(scale_factor = 2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        en_layer1 = F.relu(self.conv1(x))   \n",
    "        en_layer1 = self.pool(en_layer1)\n",
    "        en_layer2 = F.relu(self.conv2(en_layer1))\n",
    "        en_layer2 = self.pool(en_layer2)\n",
    "        en_layer3 = F.relu(self.conv3(en_layer2))\n",
    "        en_layer3 = self.pool(en_layer3)\n",
    "\n",
    "        #de_layer0 = F.relu(self.t_conv0(en_layer4)) \n",
    "        de_layer1 = F.relu(self.t_conv1(en_layer3))   \n",
    "        #de_layer1 = nn.functional.interpolate(de_layer1,scale_factor = 2)\n",
    "        de_layer2= F.relu(self.t_conv2(de_layer1)) \n",
    "        de_layer3= F.relu(self.t_conv3(de_layer2))\n",
    "        de_layer4= F.relu(self.t_conv4(de_layer3))\n",
    "        de_layer5= F.relu(self.t_conv5(de_layer4))\n",
    "        #de_layer6= F.relu(self.t_conv6(de_layer5))\n",
    "        \n",
    "        #de_layerout = torch.sigmoid(self.t_conv3(de_layer2))\n",
    "        de_layerout = F.relu(self.t_convout(de_layer5))\n",
    "        #de_layerout = self.t_conv5(de_layer4)\n",
    "        \n",
    "        if self.layer_disp:\n",
    "            print(\"input\", x.shape)\n",
    "            print(\"en_layer1\",en_layer1.shape)\n",
    "            #print(\"en_layer1a\",en_layer1a.shape)\n",
    "            print(\"en_layer2\",en_layer2.shape)\n",
    "            #print(\"en_layer2a\",en_layer2a.shape)\n",
    "            print(\"en_layer3\",en_layer3.shape)\n",
    "            #print(\"en_layer3a\",en_layer3a.shape)\n",
    "            #print(\"en_layer4\",en_layer4.shape)\n",
    "            #print(\"en_layer5\",en_layer5.shape)\n",
    "            \n",
    "            #print(\"de_layer0\",de_layer0.shape)\n",
    "            print(\"de_layer1\",de_layer1.shape)\n",
    "            #print(\"de_layer1a\",de_layer1a.shape)\n",
    "            print(\"de_layer2\",de_layer2.shape)\n",
    "            print(\"de_layer3\",de_layer3.shape)\n",
    "            print(\"de_layer4\",de_layer4.shape)\n",
    "            print(\"de_layer5\",de_layer5.shape)\n",
    "            #print(\"de_layer6\",de_layer6.shape)\n",
    "            #print(\"de_layer2a\",de_layer2a.shape)\n",
    "            #print(\"de_layer3\",de_layerou.shape)\n",
    "            #print(\"de_layer3a\",de_layer3a.shape)\n",
    "            #print(\"de_layer4\",de_layer4.shape)\n",
    "            print(\"de_layerout\",de_layerout.shape)\n",
    "              \n",
    "        return de_layerout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "179b2f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([3, 128, 128])\n",
      "en_layer1 torch.Size([128, 64, 64])\n",
      "en_layer2 torch.Size([256, 16, 16])\n",
      "en_layer3 torch.Size([256, 2, 2])\n",
      "de_layer1 torch.Size([128, 4, 4])\n",
      "de_layer2 torch.Size([128, 8, 8])\n",
      "de_layer3 torch.Size([64, 16, 16])\n",
      "de_layer4 torch.Size([64, 32, 32])\n",
      "de_layer5 torch.Size([32, 64, 64])\n",
      "de_layerout torch.Size([3, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parth/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1003.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "#Now to intialise the model and shit\n",
    "model = ConvAutoencoder(layer_disp = False).to(device)\n",
    "model = ConvAutoencoder(layer_disp = True).to(device)\n",
    "\n",
    "#Defining the loss function between the input and the output\n",
    "#criterion = nn.BCELoss()\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.L1Loss()\n",
    "#Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1*1e-3)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 50, gamma=0.5)\n",
    "\n",
    "#This is just for testing and seeing the outputs of each convolutional layer\n",
    "dummy = model(torch.empty(3, 128, 128).to(device))\n",
    "#from torchviz import make_dot\n",
    "#make_dot(dummy, params=dict(list(model.named_parameters()))).render(\"autoencoder\", format=\"png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc309965",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa9d813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from livelossplot import PlotLosses\n",
    "\n",
    "# n_epochs = 10000 #Number of epochs\n",
    "\n",
    "# liveloss = PlotLosses()\n",
    "# logs = {}\n",
    "\n",
    "\n",
    "# for epoch in range(1, n_epochs+1):\n",
    "#     # monitor training loss\n",
    "#     train_loss = 0.0\n",
    "\n",
    "#     #Training\n",
    "#     for data in train_loader:\n",
    "#         #For some reason this is needed\n",
    "#         output = data\n",
    "#         #print(output)\n",
    "#         images = output['data']\n",
    "#         labels = output['label']\n",
    "#         #images, _ = data #Don't care about the labels\n",
    "#         images = images.to(device) \n",
    "#         labels = labels.to(device) \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(images)\n",
    "#         #outputs = model(labels) #to test if the autoencoder works\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_loss += loss.item()*images.size(0)\n",
    "          \n",
    "#     train_loss = train_loss/len(train_loader)\n",
    "#     logs['loss'] = train_loss\n",
    "    \n",
    "#     #print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n",
    "#     predictions = outputs\n",
    "#     predictions = predictions.detach().cpu().numpy()\n",
    "#     imshow(predictions[0])\n",
    "#     if not(epoch%50) or epoch ==0:\n",
    "#         plt.show()\n",
    "\n",
    "#     liveloss.update(logs)\n",
    "#     liveloss.send()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db8086d",
   "metadata": {},
   "source": [
    "# Display the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ea27f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([39, 3, 128, 128])\n",
      "en_layer1 torch.Size([39, 128, 64, 64])\n",
      "en_layer2 torch.Size([39, 256, 16, 16])\n",
      "en_layer3 torch.Size([39, 256, 2, 2])\n",
      "de_layer1 torch.Size([39, 128, 4, 4])\n",
      "de_layer2 torch.Size([39, 128, 8, 8])\n",
      "de_layer3 torch.Size([39, 64, 16, 16])\n",
      "de_layer4 torch.Size([39, 64, 32, 32])\n",
      "de_layer5 torch.Size([39, 32, 64, 64])\n",
      "de_layerout torch.Size([39, 3, 128, 128])\n",
      "(39, 3, 128, 128)\n",
      "torch.Size([39, 3, 128, 128])\n",
      "Sensor Input\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 39 is out of bounds for axis 0 with size 39",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 38\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39m#fig, axes = plt.subplots(nrows=1, ncols=8, sharex=True, sharey=True, figsize=(12,4))\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39marange(\u001b[39m40\u001b[39m):\n\u001b[1;32m     35\u001b[0m     \u001b[39m#ax = fig.add_subplot(1, 8, idx+1, xticks=[], yticks=[])\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[39m#imshow(images[idx])\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtranspose(images[idx], (\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m))\n\u001b[1;32m     39\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39m255\u001b[39m\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, dtype \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39muint8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(\u001b[39minput\u001b[39m, cv2\u001b[39m.\u001b[39mCOLOR_RGB2BGR)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 39 is out of bounds for axis 0 with size 39"
     ]
    }
   ],
   "source": [
    "\n",
    "#Basically load the created class in\n",
    "test_dataset = SimData(\"test\")\n",
    "\n",
    "model.load_state_dict(torch.load(\"/home/parth/UROP_2023/deblur.pth\", map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=40, num_workers=0, shuffle = False)\n",
    "#Batch of test images\n",
    "#Batch of test images\n",
    "with torch.no_grad():\n",
    "    dataiter = iter(test_loader)\n",
    "    #images, labels = next(dataiter)\n",
    "    output = next(dataiter)\n",
    "    images = output['data'].to(device) #these are the inputs\n",
    "    labels = output['label'].to(device)\n",
    "    #labels = output['label'].numpy() #again labels aren't needed\n",
    "\n",
    "    #Sample outputs\n",
    "    #predictions = model(labels) #test just the atoencoder ibit\n",
    "    predictions = model(images) #output of the network\n",
    "    images = images.detach().cpu().numpy() #convert the images back to numpy arrays\n",
    "    labels = labels.detach().cpu().numpy() #convert the images back to numpy arrays\n",
    "\n",
    "    #convert back to appropriate numpy array and shit\n",
    "    print(images.shape)\n",
    "    print(predictions.shape)\n",
    "    predictions = predictions.detach().cpu().numpy()\n",
    "    #predictions = predictions.view(batch_size, 3, 32, 32)\n",
    "\n",
    "\n",
    "    #Original Images\n",
    "    print(\"Sensor Input\")\n",
    "    #fig, axes = plt.subplots(nrows=1, ncols=8, sharex=True, sharey=True, figsize=(12,4))\n",
    "    for idx in np.arange(39):\n",
    "        #ax = fig.add_subplot(1, 8, idx+1, xticks=[], yticks=[])\n",
    "        #imshow(images[idx])\n",
    "\n",
    "        input = np.transpose(images[idx], (1, 2, 0))\n",
    "        input = np.array(255*input, dtype = 'uint8')\n",
    "        input = cv2.cvtColor(input, cv2.COLOR_RGB2BGR)\n",
    "        os.makedirs('inputs', exist_ok=True) \n",
    "        base_path = os.path.join('inputs',\"im{}.jpg\".format(idx)) #training outputs\n",
    "        cv2.imwrite(base_path,input)\n",
    "        #ax.set_title(classes[labels[idx]])\n",
    "    plt.show()\n",
    "    \n",
    "    # #Original Images\n",
    "    # print(\"Real Shape Images\")\n",
    "    # fig, axes = plt.subplots(nrows=1, ncols=5, sharex=True, sharey=True, figsize=(12,4))\n",
    "    # for idx in np.arange(5):\n",
    "    #     ax = fig.add_subplot(1, 5, idx+1, xticks=[], yticks=[])\n",
    "    #     imshow(labels[idx])\n",
    "    #     #ax.set_title(classes[labels[idx]])\n",
    "    # plt.show()\n",
    "\n",
    "    #Reconstructed Images\n",
    "    print('Reconstructed Predicted Shape Images')\n",
    "    #fig, axes = plt.subplots(nrows=1, ncols=8, sharex=True, sharey=True, figsize=(12,4))\n",
    "    for idx in np.arange(39):\n",
    "        #ax = fig.add_subplot(1, 8, idx+1, xticks=[], yticks=[])\n",
    "        #imshow(predictions[idx])\n",
    "\n",
    "        pred = np.transpose(predictions[idx], (1, 2, 0))\n",
    "        pred = np.array(255*pred, dtype = 'uint8')\n",
    "        pred = cv2.cvtColor(pred, cv2.COLOR_RGB2BGR)\n",
    "        os.makedirs('predictions', exist_ok=True) \n",
    "        base_path = os.path.join('predictions',\"im{}.jpg\".format(idx)) #training outputs\n",
    "       \n",
    "        cv2.imwrite(base_path,pred) \n",
    "        #ax.set_title(classes[labels[idx]])\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8565695",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'appengine' from 'urllib3.contrib' (/home/parth/.local/lib/python3.10/site-packages/urllib3/contrib/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/requests_toolbelt/_compat.py:48\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mrequests\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpackages\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39murllib3\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontrib\u001b[39;00m \u001b[39mimport\u001b[39;00m appengine \u001b[39mas\u001b[39;00m gaecontrib\n\u001b[1;32m     49\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'appengine' from 'requests.packages.urllib3.contrib' (/home/parth/.local/lib/python3.10/site-packages/urllib3/contrib/__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mroboflow\u001b[39;00m \u001b[39mimport\u001b[39;00m Roboflow\n\u001b[1;32m      2\u001b[0m rf \u001b[39m=\u001b[39m Roboflow(api_key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mqnNaV5QQtZevwcJxsA5Y\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m project \u001b[39m=\u001b[39m rf\u001b[39m.\u001b[39mworkspace()\u001b[39m.\u001b[39mproject(\u001b[39m\"\u001b[39m\u001b[39mdigit-braille\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/roboflow/__init__.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mroboflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m API_URL, APP_URL, DEMO_KEYS, load_roboflow_api_key\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mroboflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mproject\u001b[39;00m \u001b[39mimport\u001b[39;00m Project\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mroboflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mworkspace\u001b[39;00m \u001b[39mimport\u001b[39;00m Workspace\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mroboflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgeneral\u001b[39;00m \u001b[39mimport\u001b[39;00m write_line\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/roboflow/core/project.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image, UnidentifiedImageError\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrequests_toolbelt\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmultipart\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mencoder\u001b[39;00m \u001b[39mimport\u001b[39;00m MultipartEncoder\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mroboflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m API_URL, DEFAULT_BATCH_NAME, DEMO_KEYS\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mroboflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m Version\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/requests_toolbelt/__init__.py:12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mrequests-toolbelt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m=================\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m:license: Apache v2.0, see LICENSE for more details\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39madapters\u001b[39;00m \u001b[39mimport\u001b[39;00m SSLAdapter, SourceAddressAdapter\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mauth\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mguess\u001b[39;00m \u001b[39mimport\u001b[39;00m GuessAuth\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmultipart\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     MultipartEncoder, MultipartEncoderMonitor, MultipartDecoder,\n\u001b[1;32m     16\u001b[0m     ImproperBodyPartContentException, NonMultipartContentTypeException\n\u001b[1;32m     17\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/requests_toolbelt/adapters/__init__.py:12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mrequests-toolbelt.adapters\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m==========================\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m:license: Apache v2.0, see LICENSE for more details\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mssl\u001b[39;00m \u001b[39mimport\u001b[39;00m SSLAdapter\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39msource\u001b[39;00m \u001b[39mimport\u001b[39;00m SourceAddressAdapter\n\u001b[1;32m     15\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mSSLAdapter\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSourceAddressAdapter\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/requests_toolbelt/adapters/ssl.py:16\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrequests\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39madapters\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPAdapter\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_compat\u001b[39;00m \u001b[39mimport\u001b[39;00m poolmanager\n\u001b[1;32m     19\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mSSLAdapter\u001b[39;00m(HTTPAdapter):\n\u001b[1;32m     20\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m    A HTTPS Adapter for Python Requests that allows the choice of the SSL/TLS\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m    version negotiated by Requests. This can be used either to enforce the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39m    properly when used with proxies.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/requests_toolbelt/_compat.py:50\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[39mfrom\u001b[39;00m \u001b[39mrequests\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpackages\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39murllib3\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontrib\u001b[39;00m \u001b[39mimport\u001b[39;00m appengine \u001b[39mas\u001b[39;00m gaecontrib\n\u001b[1;32m     49\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m         \u001b[39mfrom\u001b[39;00m \u001b[39murllib3\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontrib\u001b[39;00m \u001b[39mimport\u001b[39;00m appengine \u001b[39mas\u001b[39;00m gaecontrib\n\u001b[1;32m     52\u001b[0m \u001b[39mif\u001b[39;00m requests\u001b[39m.\u001b[39m__build__ \u001b[39m<\u001b[39m \u001b[39m0x021200\u001b[39m:\n\u001b[1;32m     53\u001b[0m     PyOpenSSLContext \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'appengine' from 'urllib3.contrib' (/home/parth/.local/lib/python3.10/site-packages/urllib3/contrib/__init__.py)"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"qnNaV5QQtZevwcJxsA5Y\")\n",
    "project = rf.workspace().project(\"digit-braille\")\n",
    "classifier = project.version(3).model\n",
    "\n",
    "number = 0\n",
    "\n",
    "# infer on a local image\n",
    "print(classifier.predict(\"/home/parth/UROP_2023/autoencoder/inputs/im{}.jpg\".format(number), confidence=10, overlap=30).json())\n",
    "\n",
    "# visualize your prediction\n",
    "classifier.predict(\"/home/parth/UROP_2023/autoencoder/inputs/im{}.jpg\".format(number), confidence=10, overlap=30).save(\"prediction.jpg\")\n",
    "\n",
    "# infer on an image hosted elsewhere\n",
    "# print(model.predict(\"URL_OF_YOUR_IMAGE\", hosted=True, confidence=40, overlap=30).json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
