{
 "cells": [
  {
   "cell_type": "raw",
   "id": "24b61755",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd318a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "\n",
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785e9ccd",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7136089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for showing images\n",
    "def imshow(img):\n",
    "    #img = img / 2 + 0.5  \n",
    "    #print(img)\n",
    "    #plt.imshow can take in images with 0-1 (floats) or 0-255 (int)\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0))) \n",
    "    #print(img)\n",
    "    #print(max(img[0][0]))\n",
    "    print(np.mean(img[0][2]))\n",
    "    print(np.std(img[0][2]))\n",
    "    #print(max(img[0][1]))\n",
    "    #print(max(img[0][2]))\n",
    "#It's best to keep the data processing code separate from the class loader\n",
    "def to_tensor_and_normalize(imagepil): #Done with testing\n",
    "    \"\"\"Convert image to torch Tensor and normalize using the ImageNet training\n",
    "    set mean and stdev taken from\n",
    "    https://pytorch.org/docs/stable/torchvision/models.html.\n",
    "    Why the ImageNet mean and stdev instead of the PASCAL VOC mean and stdev?\n",
    "    Because we are using a model pretrained on ImageNet.\"\"\"\n",
    "    #Think the reason for introducing normalisation is because of the imagenet weights\n",
    "    #ChosenTransforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "    #            torchvision.transforms.Normalize(mean=[0.6236, 0.5118, 0.4264],std=[0.3545, 0.2692, 0.3376]),])\n",
    "\n",
    "\n",
    "    #This straight up just transforms 0-255 to 0-1\n",
    "    ChosenTransforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])#, torchvision.transforms.Normalize(mean=[0.3, 0.3, 0.3],std=[0.05, 0.05, 0.05])])\n",
    "    #ChosenTransforms = torchvision.transforms.Compose([torchvision.transforms.PILToTensor()])#[torchvision.transforms.ToTensor()])\n",
    "    \"\"\"ChosenTransforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                                      lambda x: x*1.666666666666666666667,\n",
    "                                                      ])\"\"\"\n",
    "    return ChosenTransforms(imagepil)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e00af3d",
   "metadata": {},
   "source": [
    "# Class for loading in my simulated image training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79d16792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimData(Dataset):\n",
    "    def __init__(self, setname):\n",
    "        '''The sim input images and output images are 128 x 128 x 3'''\n",
    "        self.setname = setname\n",
    "        assert setname in ['train','val','test']\n",
    "        \n",
    "        #Define where to load in the dataset\n",
    "        overall_dataset_dir = os.path.join(os.path.join(os.getcwd(),'load_dataset'), 'data')\n",
    "        #input images\n",
    "        self.selected_dataset_dir = os.path.join(overall_dataset_dir,setname)\n",
    "        \n",
    "        #output images\n",
    "        self.selected_dataset_output_dir = os.path.join(overall_dataset_dir,setname+\"_outputs\")\n",
    "\n",
    "        count = 0\n",
    "        # Iterate directory\n",
    "        for path in os.listdir(self.selected_dataset_dir): #get number of images in folder\n",
    "            # check if current path is a file\n",
    "            if os.path.isfile(os.path.join(self.selected_dataset_dir, path)):\n",
    "                count += 1 # increment count\n",
    "        \n",
    "        #E.g. self.all_filenames = ['im1.jpg',..,'imN.jpg'] when setname=='train'\n",
    "        #Loads in the input images from the training folder\n",
    "        self.all_filenames = [] #initalise list of filenames\n",
    "        self.all_filenames_output = [] \n",
    "        for i in range(1,count+1):  #iterate through all the images by number rather than os.listdir which does it randomly\n",
    "            self.all_filenames.append(\"im\"+str(i)+\".jpg\")\n",
    "            self.all_filenames_output.append(\"im\"+str(i)+\".jpg\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of examples in this split, e.g. if\n",
    "        self.setname=='train' then return the total number of examples\n",
    "        in the training set\"\"\"\n",
    "        return len(self.all_filenames)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \"\"\"Return the example at index [idx]. The example is a dict with keys\n",
    "        'data' (value: Tensor for an RGB image) and labels are also images\"\"\"\n",
    "        #For the inputs\n",
    "        selected_filename = self.all_filenames[idx]\n",
    "        #print(selected_filename)\n",
    "        #test= self.all_filenames_output[idx]\n",
    "        #print(test)\n",
    "        imagepil = PIL.Image.open(os.path.join(self.selected_dataset_dir,selected_filename)).convert('RGB')\n",
    "        \n",
    "        #For the outputs\n",
    "        selected_filename_output = self.all_filenames_output[idx]\n",
    "        imagepil_output = PIL.Image.open(os.path.join(self.selected_dataset_output_dir,\n",
    "                                                      selected_filename_output)).convert('RGB')\n",
    "        \n",
    "        #convert image to Tensor/normalize\n",
    "        image = to_tensor_and_normalize(imagepil)\n",
    "        image_output = to_tensor_and_normalize(imagepil_output)\n",
    "        \n",
    "        \n",
    "        sample = {'data':image, #preprocessed image, for input into NN\n",
    "                  'label':image_output,\n",
    "                  'img_idx':idx}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754edf6c",
   "metadata": {},
   "source": [
    "# Now to load in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73076b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Basically load the created class in\n",
    "# train_dataset = SimData(\"train\")\n",
    "\n",
    "# #Prepare data loaders\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, num_workers=0, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0068bdf",
   "metadata": {},
   "source": [
    "# Display the loaded in images for a sanity check lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2447c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataiter = iter(train_loader)\n",
    "# #For some reason this is needed\n",
    "# output = next(dataiter)\n",
    "# images = output['data'].numpy()\n",
    "# labels = output['label'].numpy()\n",
    "# #print(labels)\n",
    "# #Rather than this\n",
    "# #images, labels, _ = next(dataiter)\n",
    "# #images = images.numpy() # convert images to numpy for display\n",
    "\n",
    "\n",
    "# #Plot the images\n",
    "# print(\"Input Images (Sensor Data)\")\n",
    "# fig, axes = plt.subplots(nrows=1, ncols=5, sharex=True, sharey=True, figsize=(12,4))\n",
    "# for idx in np.arange(5):\n",
    "#     ax = fig.add_subplot(1, 5, idx+1, xticks=[], yticks=[])\n",
    "#     imshow(images[idx])\n",
    "#     #ax.set_title(classes[labels[idx]])\n",
    "# plt.show()\n",
    "\n",
    "# #Output images\n",
    "# print(\"Shape output Images\")\n",
    "# fig, axes = plt.subplots(nrows=1, ncols=5, sharex=True, sharey=True, figsize=(12,4))\n",
    "# for idx in np.arange(5):\n",
    "#     ax = fig.add_subplot(1, 5, idx+1, xticks=[], yticks=[])\n",
    "#     imshow(labels[idx])\n",
    "#     #ax.set_title(classes[labels[idx]])\n",
    "# plt.show() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e6a8486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(images[0].shape)\n",
    "# print(labels[0].shape)\n",
    "# print(images[0])\n",
    "# print(np.max(images[0][1]))\n",
    "# print(np.mean(images[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96914298",
   "metadata": {},
   "source": [
    "#  Conv autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec92c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, layer_disp = False):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        self.layer_disp = layer_disp\n",
    "        #Encoder\n",
    "        #nn.Conv2(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, \n",
    "                               out_channels=128,\n",
    "                               kernel_size=2, \n",
    "                               stride = 1,\n",
    "                               padding = 'same')\n",
    "        self.conv2 = nn.Conv2d(in_channels=128, \n",
    "                               out_channels=256,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding = 0)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=256, \n",
    "                               out_channels=256,\n",
    "                               kernel_size=4, \n",
    "                               stride = 4,\n",
    "                               padding = 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        \n",
    "       \n",
    "        #Decoder\n",
    "        #nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.t_conv1 = nn.ConvTranspose2d(in_channels=256, \n",
    "                               out_channels=128,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)\n",
    "        self.t_conv2= nn.ConvTranspose2d(in_channels=128, \n",
    "                               out_channels=128,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)\n",
    "        self.t_conv3= nn.ConvTranspose2d(in_channels=128, \n",
    "                               out_channels=64,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)\n",
    "        \n",
    "        self.t_conv4= nn.ConvTranspose2d(in_channels=64, \n",
    "                               out_channels=64,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)\n",
    "        self.t_conv5= nn.ConvTranspose2d(in_channels=64, \n",
    "                               out_channels=32,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)\n",
    "        '''self.t_conv6= nn.ConvTranspose2d(in_channels=32, \n",
    "                               out_channels=32,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)'''\n",
    "       \n",
    "        self.t_convout = nn.ConvTranspose2d(in_channels=32, \n",
    "                               out_channels=3,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)\n",
    "        #self.upsample = nn.functional.interpolate(scale_factor = 2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        en_layer1 = F.relu(self.conv1(x))   \n",
    "        en_layer1 = self.pool(en_layer1)\n",
    "        en_layer2 = F.relu(self.conv2(en_layer1))\n",
    "        en_layer2 = self.pool(en_layer2)\n",
    "        en_layer3 = F.relu(self.conv3(en_layer2))\n",
    "        en_layer3 = self.pool(en_layer3)\n",
    "\n",
    "        #de_layer0 = F.relu(self.t_conv0(en_layer4)) \n",
    "        de_layer1 = F.relu(self.t_conv1(en_layer3))   \n",
    "        #de_layer1 = nn.functional.interpolate(de_layer1,scale_factor = 2)\n",
    "        de_layer2= F.relu(self.t_conv2(de_layer1)) \n",
    "        de_layer3= F.relu(self.t_conv3(de_layer2))\n",
    "        de_layer4= F.relu(self.t_conv4(de_layer3))\n",
    "        de_layer5= F.relu(self.t_conv5(de_layer4))\n",
    "        #de_layer6= F.relu(self.t_conv6(de_layer5))\n",
    "        \n",
    "        #de_layerout = torch.sigmoid(self.t_conv3(de_layer2))\n",
    "        de_layerout = F.relu(self.t_convout(de_layer5))\n",
    "        #de_layerout = self.t_conv5(de_layer4)\n",
    "        \n",
    "        if self.layer_disp:\n",
    "            print(\"input\", x.shape)\n",
    "            print(\"en_layer1\",en_layer1.shape)\n",
    "            #print(\"en_layer1a\",en_layer1a.shape)\n",
    "            print(\"en_layer2\",en_layer2.shape)\n",
    "            #print(\"en_layer2a\",en_layer2a.shape)\n",
    "            print(\"en_layer3\",en_layer3.shape)\n",
    "            #print(\"en_layer3a\",en_layer3a.shape)\n",
    "            #print(\"en_layer4\",en_layer4.shape)\n",
    "            #print(\"en_layer5\",en_layer5.shape)\n",
    "            \n",
    "            #print(\"de_layer0\",de_layer0.shape)\n",
    "            print(\"de_layer1\",de_layer1.shape)\n",
    "            #print(\"de_layer1a\",de_layer1a.shape)\n",
    "            print(\"de_layer2\",de_layer2.shape)\n",
    "            print(\"de_layer3\",de_layer3.shape)\n",
    "            print(\"de_layer4\",de_layer4.shape)\n",
    "            print(\"de_layer5\",de_layer5.shape)\n",
    "            #print(\"de_layer6\",de_layer6.shape)\n",
    "            #print(\"de_layer2a\",de_layer2a.shape)\n",
    "            #print(\"de_layer3\",de_layerou.shape)\n",
    "            #print(\"de_layer3a\",de_layer3a.shape)\n",
    "            #print(\"de_layer4\",de_layer4.shape)\n",
    "            print(\"de_layerout\",de_layerout.shape)\n",
    "              \n",
    "        return de_layerout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "179b2f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([3, 128, 128])\n",
      "en_layer1 torch.Size([128, 64, 64])\n",
      "en_layer2 torch.Size([256, 16, 16])\n",
      "en_layer3 torch.Size([256, 2, 2])\n",
      "de_layer1 torch.Size([128, 4, 4])\n",
      "de_layer2 torch.Size([128, 8, 8])\n",
      "de_layer3 torch.Size([64, 16, 16])\n",
      "de_layer4 torch.Size([64, 32, 32])\n",
      "de_layer5 torch.Size([32, 64, 64])\n",
      "de_layerout torch.Size([3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "#Now to intialise the model \n",
    "model = ConvAutoencoder(layer_disp = False).to(device)\n",
    "model = ConvAutoencoder(layer_disp = True).to(device)\n",
    "\n",
    "#Defining the loss function between the input and the output\n",
    "#criterion = nn.BCELoss()\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.L1Loss()\n",
    "#Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1*1e-3)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 50, gamma=0.5)\n",
    "\n",
    "#This is just for testing and seeing the outputs of each convolutional layer\n",
    "dummy = model(torch.empty(3, 128, 128).to(device))\n",
    "#from torchviz import make_dot\n",
    "#make_dot(dummy, params=dict(list(model.named_parameters()))).render(\"autoencoder\", format=\"png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc309965",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa9d813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from livelossplot import PlotLosses\n",
    "\n",
    "# n_epochs = 10000 #Number of epochs\n",
    "\n",
    "# liveloss = PlotLosses()\n",
    "# logs = {}\n",
    "\n",
    "\n",
    "# for epoch in range(1, n_epochs+1):\n",
    "#     # monitor training loss\n",
    "#     train_loss = 0.0\n",
    "\n",
    "#     #Training\n",
    "#     for data in train_loader:\n",
    "#         #For some reason this is needed\n",
    "#         output = data\n",
    "#         #print(output)\n",
    "#         images = output['data']\n",
    "#         labels = output['label']\n",
    "#         #images, _ = data #Don't care about the labels\n",
    "#         images = images.to(device) \n",
    "#         labels = labels.to(device) \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(images)\n",
    "#         #outputs = model(labels) #to test if the autoencoder works\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_loss += loss.item()*images.size(0)\n",
    "          \n",
    "#     train_loss = train_loss/len(train_loader)\n",
    "#     logs['loss'] = train_loss\n",
    "    \n",
    "#     #print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n",
    "#     predictions = outputs\n",
    "#     predictions = predictions.detach().cpu().numpy()\n",
    "#     imshow(predictions[0])\n",
    "#     if not(epoch%50) or epoch ==0:\n",
    "#         plt.show()\n",
    "\n",
    "#     liveloss.update(logs)\n",
    "#     liveloss.send()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db8086d",
   "metadata": {},
   "source": [
    "# Display the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ea27f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([39, 3, 128, 128])\n",
      "en_layer1 torch.Size([39, 128, 64, 64])\n",
      "en_layer2 torch.Size([39, 256, 16, 16])\n",
      "en_layer3 torch.Size([39, 256, 2, 2])\n",
      "de_layer1 torch.Size([39, 128, 4, 4])\n",
      "de_layer2 torch.Size([39, 128, 8, 8])\n",
      "de_layer3 torch.Size([39, 64, 16, 16])\n",
      "de_layer4 torch.Size([39, 64, 32, 32])\n",
      "de_layer5 torch.Size([39, 32, 64, 64])\n",
      "de_layerout torch.Size([39, 3, 128, 128])\n",
      "(39, 3, 128, 128)\n",
      "torch.Size([39, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# #Basically load the created class in\n",
    "# test_dataset = SimData(\"test\")\n",
    "\n",
    "# model.load_state_dict(torch.load(\"/home/parth/UROP_2023/deblur.pth\", map_location=torch.device('cpu')))\n",
    "# model.eval()\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=40, num_workers=0, shuffle = False)\n",
    "\n",
    "# #Batch of test images\n",
    "# with torch.no_grad():\n",
    "#     dataiter = iter(test_loader)\n",
    "#     #images, labels = next(dataiter)\n",
    "#     output = next(dataiter)\n",
    "#     images = output['data'].to(device) #these are the inputs\n",
    "#     labels = output['label'].to(device)\n",
    "#     #labels = output['label'].numpy() #again labels aren't needed\n",
    "\n",
    "#     #Sample outputs\n",
    "#     #predictions = model(labels) #test just the atoencoder ibit\n",
    "#     predictions = model(images) #output of the network\n",
    "#     images = images.detach().cpu().numpy() #convert the images back to numpy arrays\n",
    "#     labels = labels.detach().cpu().numpy() #convert the images back to numpy arrays\n",
    "\n",
    "#     #convert back to appropriate numpy array and shit\n",
    "#     print(images.shape)\n",
    "#     print(predictions.shape)\n",
    "#     predictions = predictions.detach().cpu().numpy()\n",
    "#     #predictions = predictions.view(batch_size, 3, 32, 32)\n",
    "\n",
    "#     for idx in np.arange(len(test_dataset)): \n",
    "#         #save input images\n",
    "#         input = np.transpose(images[idx], (1, 2, 0)) #convert to correct format\n",
    "#         input = np.array(255*input, dtype = 'uint8') \n",
    "#         input = cv2.cvtColor(input, cv2.COLOR_RGB2BGR) \n",
    "#         os.makedirs('inputs', exist_ok=True) \n",
    "#         base_path = os.path.join('inputs',\"im{}.jpg\".format(idx+1)) \n",
    "#         cv2.imwrite(base_path,input)\n",
    "\n",
    "#         #save output images\n",
    "#         pred = np.transpose(predictions[idx], (1, 2, 0)) #convert to correct format\n",
    "#         pred = np.array(255*pred, dtype = 'uint8')\n",
    "#         pred = cv2.cvtColor(pred, cv2.COLOR_RGB2BGR)\n",
    "#         os.makedirs('predictions', exist_ok=True) \n",
    "#         base_path = os.path.join('predictions',\"im{}.jpg\".format(idx+1))\n",
    "#         cv2.imwrite(base_path,pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68b443b",
   "metadata": {},
   "source": [
    "# Testing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8565695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "kzbsi   a l mmxuppd p  wnnlpmxnncaaekeooo\n"
     ]
    }
   ],
   "source": [
    "##############################################################################################################\n",
    "#import libraries\n",
    "\n",
    "import cv2\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "from threading import Thread\n",
    "import re\n",
    "import json\n",
    "from roboflow import Roboflow\n",
    "\n",
    "sys.path.insert(0, '/home/parth/UROP_2023/Generic_ur5_controller') #add path to brailley controller\n",
    "from digit_demo import DigitSensor, DisplayImage\n",
    "import kg_robot as kgr\n",
    "##############################################################################################################\n",
    "#get target text file properties\n",
    "\n",
    "props = []\n",
    "with open('test_properties.csv', 'r') as f:\n",
    "    props = f.read()\n",
    "print(props)\n",
    "word_count = int(props[0])\n",
    "letter_count = int(props[1])\n",
    "line_count = int(props[2])\n",
    "##############################################################################################################\n",
    "#read target text file on braille reader\n",
    "\n",
    "total_rows = line_count  #found from target text file\n",
    "dynamic_count = 1\n",
    "row_counter = 1\n",
    "\n",
    "z_depth = 0.0143 #set z depth of sensor, with medical tape need to be lower for clarity\n",
    "y_offset = -0.27 #set y offset of sensor\n",
    "\n",
    "velocity = 0.2 #initialise velocity\n",
    "start = 0\n",
    "end = 0\n",
    "slide_capture_flag = False #initialise slide capture flag\n",
    "\n",
    "with (DigitSensor(serialno='D20654', resolution='QVGA', framerate='60') as digit,\n",
    "            DisplayImage(window_name='DIGIT Demo') as display): #use wrapper to make accessing DIGIT sensor easier\n",
    "    frame = digit.get_frame()\n",
    "    print(\"------------Configuring brailley------------\\r\\n\")\n",
    "    brailley = kgr.kg_robot(port=30000,db_host=\"169.254.252.50\")\n",
    "    print(\"----------------Hi brailley!----------------\\r\\n\\r\\n\")\n",
    "\n",
    "    def read_camera(): #function to read camera\n",
    "         global frame\n",
    "         global start\n",
    "         while True:\n",
    "            frame = digit.get_frame() #get frame from camera\n",
    "            if slide_capture_flag == True: \n",
    "                capture_frame(\"test_blurry\") #capture frame\n",
    "\n",
    "    def capture_frame(dir_path): #function to capture frame and save it\n",
    "        global static_collect, dynamic_collect, static_count, dynamic_count\n",
    "        os.makedirs(dir_path, exist_ok=True)  \n",
    "        base_path = os.path.join(dir_path,\"im{}.jpg\".format(dynamic_count)) #create path to save frame\n",
    "        cv2.imwrite(base_path, frame)\n",
    "        dynamic_count += 1 #increment counts\n",
    "    \n",
    "    def move_robot(): #fixed movements for each data collection step\n",
    "        global dynamic_count, velocity, row_counter\n",
    "        global slide_capture_flag\n",
    "        global start\n",
    "\n",
    "        slide_capture_flag = True\n",
    "        brailley.movel([0.296, y_offset, z_depth,  2.21745, 2.22263, -0.00201733], 500, velocity) #slide across one row\n",
    "        slide_capture_flag = False\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        row_counter += 1 #increment row counter\n",
    "        scroll_button() #press scroll button\n",
    "\n",
    "    def scroll_button():\n",
    "        brailley.movel([0.305801, -0.261322, 0.0186874, 2.21758, 2.22249, -0.00198903], 0.5, 0.2) #move to scroll position\n",
    "        time.sleep(0.5)\n",
    "        brailley.translatel_rel([0, 0, -0.006, 0, 0, 0], 0.5, 0.2) #press scroll button\n",
    "        time.sleep(0.5)\n",
    "        brailley.translatel_rel([0, 0, 0.006, 0, 0, 0], 0.5, 0.2) #move back to scroll position\n",
    "        time.sleep(0.5)\n",
    "        brailley.movel([0.296,y_offset, z_depth+0.01, 2.21745, 2.22263, -0.00201733], 0.5, 0.2) #move above first position\n",
    "        time.sleep(0.5)\n",
    "        brailley.movel([0.296,y_offset, z_depth, 2.21745, 2.22263, -0.00201733], 0.5, 0.2) #move to first position\n",
    "\n",
    "    if __name__=='__main__':\n",
    "        t= Thread(target=read_camera) #start thread to read camera\n",
    "        t.daemon = True #set thread to daemon so it closes when main thread closes\n",
    "        t.start()\n",
    "        \n",
    "        brailley.movel([0.169, y_offset, z_depth+0.01, 2.21745, 2.22263, -0.00101733], 0.5, 0.2) #move above first position\n",
    "        time.sleep(0.5)\n",
    "        brailley.movel([0.169, y_offset, z_depth, 2.21745, 2.22263, -0.00201733], 0.5, 0.2) #move to first position\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        print(\"------------Starting data collection------------\\r\\n\")\n",
    "        start = time.time() #start timer\n",
    "        while row_counter <= total_rows: #get at least the target data set size\n",
    "            move_robot() #movements\n",
    "            print(\"Row {} of {} collected\".format(row_counter, total_rows)) #print progress\n",
    "        end = time.time()\n",
    "        time_taken = end-start\n",
    "        wpm_speed = word_count/(time_taken/60) #calculate words per minute\n",
    "        print(\"Time taken: {} seconds\".format(time_taken)) #print time taken\n",
    "        print(\"Words: {}\".format(word_count))\n",
    "        print(\"Speed: {} words per minute\".format(wpm_speed))\n",
    "print(\"------------Data collection complete------------\\r\\n\") \n",
    "##############################################################################################################\n",
    "#process images for autoencoder\n",
    "\n",
    "os.rmdir('/home/parth/UROP_2023/autoencoder/load_dataset/data/test') #remove any pre-existing test folder\n",
    "os.rmdir('/home/parth/UROP_2023/autoencoder/load_dataset/data/test_outputs') #remove any pre-existing test_outputs folder\n",
    "\n",
    "for image in os.scandir('test_blurry'): #for each image in the blurry folder\n",
    "    path = image.path\n",
    "    num = (re.findall(r'\\d+', path))[0] #get image number\n",
    "\n",
    "    img = cv2.imread(path) #read image\n",
    "    new_size = (128, 128) # new_size=(width, height)\n",
    "    img = cv2.resize(img, new_size) \n",
    "\n",
    "    os.makedirs('/home/parth/UROP_2023/autoencoder/load_dataset/data/test', exist_ok=True) #save resized images to test folder\n",
    "    base_path = os.path.join('/home/parth/UROP_2023/autoencoder/load_dataset/data/test',\"im{}.jpg\".format(num)) \n",
    "    cv2.imwrite(base_path, img)\n",
    "\n",
    "    os.makedirs('/home/parth/UROP_2023/autoencoder/load_dataset/data/test_outputs', exist_ok=True) #need to create test_outputs folder to work with SimData class\n",
    "    base_path = os.path.join('/home/parth/UROP_2023/autoencoder/load_dataset/data/test_outputs',\"im{}.jpg\".format(num)) \n",
    "    cv2.imwrite(base_path, img)\n",
    "##############################################################################################################\n",
    "#load blurry images into autoencoder\n",
    "\n",
    "test_dataset = SimData(\"test\") #load test dataset as SimData class\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, num_workers=0, shuffle = False) #no shuffle so images go in order\n",
    "\n",
    "model.load_state_dict(torch.load(\"/home/parth/UROP_2023/deblur.pth\", map_location=torch.device('cpu'))) #load pre-trained autoencdoer model\n",
    "model.eval() #set model to evaluation mode\n",
    "\n",
    "#input blurry images into autoencoder\n",
    "with torch.no_grad():\n",
    "    dataiter = iter(test_loader)\n",
    "    output = next(dataiter)\n",
    "    images = output['data'].to(device) #these are the inputs\n",
    "    labels = output['label'].to(device)\n",
    "\n",
    "    #Sample outputs\n",
    "    predictions = model(images) #output of the network\n",
    "    images = images.detach().cpu().numpy() #convert the images back to numpy arrays\n",
    "    labels = labels.detach().cpu().numpy() #convert the images back to numpy arrays\n",
    "\n",
    "    #convert back to appropriate numpy array \n",
    "    print(images.shape)\n",
    "    print(predictions.shape)\n",
    "    predictions = predictions.detach().cpu().numpy()\n",
    "\n",
    "    for idx in np.arange(len(test_dataset)): \n",
    "        #save input images\n",
    "        input = np.transpose(images[idx], (1, 2, 0)) #convert to correct format\n",
    "        input = np.array(255*input, dtype = 'uint8') \n",
    "        input = cv2.cvtColor(input, cv2.COLOR_RGB2BGR) \n",
    "        os.makedirs('inputs', exist_ok=True) \n",
    "        base_path = os.path.join('inputs',\"im{}.jpg\".format(idx+1)) \n",
    "        cv2.imwrite(base_path,input)\n",
    "\n",
    "        #save output images\n",
    "        pred = np.transpose(predictions[idx], (1, 2, 0)) #convert to correct format\n",
    "        pred = np.array(255*pred, dtype = 'uint8')\n",
    "        pred = cv2.cvtColor(pred, cv2.COLOR_RGB2BGR)\n",
    "        os.makedirs('predictions', exist_ok=True) \n",
    "        base_path = os.path.join('predictions',\"im{}.jpg\".format(idx+1))\n",
    "        cv2.imwrite(base_path,pred) \n",
    "\n",
    "##############################################################################################################\n",
    "#input deblurred images into pre-trained YOLO v8 model\n",
    "\n",
    "rf = Roboflow(api_key=\"qnNaV5QQtZevwcJxsA5Y\") #create roboflow object\n",
    "project = rf.workspace().project(\"digit-braille\") \n",
    "classifier = project.version(3).model #load pre-trained model\n",
    "pred_text = \"\" #initialise string of predicted text\n",
    "\n",
    "os.makedirs('/home/parth/UROP_2023/autoencoder/braille_detect', exist_ok=True) #make folder to store braille classifier images\n",
    "for i in range(1, len(test_dataset)+1): #for each image in the predictions folder\n",
    "    #infer on each image\n",
    "    pred_dict = classifier.predict(\"/home/parth/UROP_2023/autoencoder/predictions/im{}.jpg\".format(i), confidence=45, overlap=25).json() #save as dictionary data type\n",
    "    if 'predictions' in pred_dict:\n",
    "        preds = pred_dict['predictions'] #get predictions from dictionary\n",
    "        if len(preds) > 0:\n",
    "            for pred in preds:\n",
    "                    pred_text += pred['class'] #add predicted character to string\n",
    "        else:\n",
    "            pred_text += \" \" #add space if no predictions\n",
    "    #save bounding box image to braille_detect folder\n",
    "    classifier.predict(\"/home/parth/UROP_2023/autoencoder/predictions/im{}.jpg\".format(i), confidence=45, overlap=25).save(\"/home/parth/UROP_2023/autoencoder/braille_detect/detect{}.jpg\".format(i))\n",
    "##############################################################################################################\n",
    "#Compare predicted text with target text\n",
    "\n",
    "pred_text = pred_text[::-1] #reverse predicted text as we are reading from right to left (to reduce effect of rolling shutter)\n",
    "print(pred_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
