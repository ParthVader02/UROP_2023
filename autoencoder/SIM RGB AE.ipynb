{
 "cells": [
  {
   "cell_type": "raw",
   "id": "24b61755",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "fd318a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "\n",
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785e9ccd",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c7136089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for showing images\n",
    "def imshow(img):\n",
    "    #img = img / 2 + 0.5  \n",
    "    #print(img)\n",
    "    #plt.imshow can take in images with 0-1 (floats) or 0-255 (int)\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0))) \n",
    "    #print(img)\n",
    "    #print(max(img[0][0]))\n",
    "    print(np.mean(img[0][2]))\n",
    "    print(np.std(img[0][2]))\n",
    "    #print(max(img[0][1]))\n",
    "    #print(max(img[0][2]))\n",
    "#It's best to keep the data processing code separate from the class loader\n",
    "def to_tensor_and_normalize(imagepil): #Done with testing\n",
    "    \"\"\"Convert image to torch Tensor and normalize using the ImageNet training\n",
    "    set mean and stdev taken from\n",
    "    https://pytorch.org/docs/stable/torchvision/models.html.\n",
    "    Why the ImageNet mean and stdev instead of the PASCAL VOC mean and stdev?\n",
    "    Because we are using a model pretrained on ImageNet.\"\"\"\n",
    "    #Think the reason for introducing normalisation is because of the imagenet weights\n",
    "    #ChosenTransforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "    #            torchvision.transforms.Normalize(mean=[0.6236, 0.5118, 0.4264],std=[0.3545, 0.2692, 0.3376]),])\n",
    "\n",
    "\n",
    "    #This straight up just transforms 0-255 to 0-1\n",
    "    ChosenTransforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])#, torchvision.transforms.Normalize(mean=[0.3, 0.3, 0.3],std=[0.05, 0.05, 0.05])])\n",
    "    #ChosenTransforms = torchvision.transforms.Compose([torchvision.transforms.PILToTensor()])#[torchvision.transforms.ToTensor()])\n",
    "    \"\"\"ChosenTransforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                                      lambda x: x*1.666666666666666666667,\n",
    "                                                      ])\"\"\"\n",
    "    return ChosenTransforms(imagepil)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e00af3d",
   "metadata": {},
   "source": [
    "# Class for loading in my simulated image training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "79d16792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimData(Dataset):\n",
    "    def __init__(self, setname):\n",
    "        '''The sim input images and output images are 128 x 128 x 3'''\n",
    "        self.setname = setname\n",
    "        assert setname in ['train','val','test']\n",
    "        \n",
    "        #Define where to load in the dataset\n",
    "        overall_dataset_dir = os.path.join(os.path.join(os.getcwd(),'load_dataset'), 'data')\n",
    "        #input images\n",
    "        self.selected_dataset_dir = os.path.join(overall_dataset_dir,setname)\n",
    "        \n",
    "        #output images\n",
    "        self.selected_dataset_output_dir = os.path.join(overall_dataset_dir,setname+\"_outputs\")\n",
    "        \n",
    "        #E.g. self.all_filenames = ['im1.jpg',..,'imN.jpg'] when setname=='train'\n",
    "        #Loads in the input images from the training folder\n",
    "        self.all_filenames = os.listdir(self.selected_dataset_dir)\n",
    "        self.all_filenames_output = os.listdir(self.selected_dataset_output_dir)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of examples in this split, e.g. if\n",
    "        self.setname=='train' then return the total number of examples\n",
    "        in the training set\"\"\"\n",
    "        return len(self.all_filenames)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \"\"\"Return the example at index [idx]. The example is a dict with keys\n",
    "        'data' (value: Tensor for an RGB image) and labels are also images\"\"\"\n",
    "        #For the inputs\n",
    "        selected_filename = self.all_filenames[idx]\n",
    "        #print(selected_filename)\n",
    "        #test= self.all_filenames_output[idx]\n",
    "        #print(test)\n",
    "        imagepil = PIL.Image.open(os.path.join(self.selected_dataset_dir,selected_filename)).convert('RGB')\n",
    "        \n",
    "        #For the outputs\n",
    "        selected_filename_output = self.all_filenames_output[idx]\n",
    "        imagepil_output = PIL.Image.open(os.path.join(self.selected_dataset_output_dir,\n",
    "                                                      selected_filename_output)).convert('RGB')\n",
    "        \n",
    "        #convert image to Tensor/normalize\n",
    "        image = to_tensor_and_normalize(imagepil)\n",
    "        image_output = to_tensor_and_normalize(imagepil_output)\n",
    "        \n",
    "        \n",
    "        sample = {'data':image, #preprocessed image, for input into NN\n",
    "                  'label':image_output,\n",
    "                  'img_idx':idx}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754edf6c",
   "metadata": {},
   "source": [
    "# Now to load in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "73076b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Basically load the created class in\n",
    "# train_dataset = SimData(\"train\")\n",
    "\n",
    "# #Prepare data loaders\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, num_workers=0, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0068bdf",
   "metadata": {},
   "source": [
    "# Display the loaded in images for a sanity check lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b2447c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataiter = iter(train_loader)\n",
    "# #For some reason this is needed\n",
    "# output = next(dataiter)\n",
    "# images = output['data'].numpy()\n",
    "# labels = output['label'].numpy()\n",
    "# #print(labels)\n",
    "# #Rather than this\n",
    "# #images, labels, _ = next(dataiter)\n",
    "# #images = images.numpy() # convert images to numpy for display\n",
    "\n",
    "\n",
    "# #Plot the images\n",
    "# print(\"Input Images (Sensor Data)\")\n",
    "# fig, axes = plt.subplots(nrows=1, ncols=5, sharex=True, sharey=True, figsize=(12,4))\n",
    "# for idx in np.arange(5):\n",
    "#     ax = fig.add_subplot(1, 5, idx+1, xticks=[], yticks=[])\n",
    "#     imshow(images[idx])\n",
    "#     #ax.set_title(classes[labels[idx]])\n",
    "# plt.show()\n",
    "\n",
    "# #Output images\n",
    "# print(\"Shape output Images\")\n",
    "# fig, axes = plt.subplots(nrows=1, ncols=5, sharex=True, sharey=True, figsize=(12,4))\n",
    "# for idx in np.arange(5):\n",
    "#     ax = fig.add_subplot(1, 5, idx+1, xticks=[], yticks=[])\n",
    "#     imshow(labels[idx])\n",
    "#     #ax.set_title(classes[labels[idx]])\n",
    "# plt.show() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "9e6a8486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(images[0].shape)\n",
    "# print(labels[0].shape)\n",
    "# print(images[0])\n",
    "# print(np.max(images[0][1]))\n",
    "# print(np.mean(images[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96914298",
   "metadata": {},
   "source": [
    "#  Conv autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "ec92c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, layer_disp = False):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        self.layer_disp = layer_disp\n",
    "        #Encoder\n",
    "        #nn.Conv2(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, \n",
    "                               out_channels=128,\n",
    "                               kernel_size=2, \n",
    "                               stride = 1,\n",
    "                               padding = 'same')\n",
    "        self.conv2 = nn.Conv2d(in_channels=128, \n",
    "                               out_channels=256,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding = 0)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=256, \n",
    "                               out_channels=256,\n",
    "                               kernel_size=4, \n",
    "                               stride = 4,\n",
    "                               padding = 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        \n",
    "       \n",
    "        #Decoder\n",
    "        #nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.t_conv1 = nn.ConvTranspose2d(in_channels=256, \n",
    "                               out_channels=128,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)\n",
    "        self.t_conv2= nn.ConvTranspose2d(in_channels=128, \n",
    "                               out_channels=128,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)\n",
    "        self.t_conv3= nn.ConvTranspose2d(in_channels=128, \n",
    "                               out_channels=64,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)\n",
    "        \n",
    "        self.t_conv4= nn.ConvTranspose2d(in_channels=64, \n",
    "                               out_channels=64,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)\n",
    "        self.t_conv5= nn.ConvTranspose2d(in_channels=64, \n",
    "                               out_channels=32,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)\n",
    "        '''self.t_conv6= nn.ConvTranspose2d(in_channels=32, \n",
    "                               out_channels=32,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)'''\n",
    "       \n",
    "        self.t_convout = nn.ConvTranspose2d(in_channels=32, \n",
    "                               out_channels=3,\n",
    "                               kernel_size=2, \n",
    "                               stride = 2,\n",
    "                               padding=0)\n",
    "        #self.upsample = nn.functional.interpolate(scale_factor = 2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        en_layer1 = F.relu(self.conv1(x))   \n",
    "        en_layer1 = self.pool(en_layer1)\n",
    "        en_layer2 = F.relu(self.conv2(en_layer1))\n",
    "        en_layer2 = self.pool(en_layer2)\n",
    "        en_layer3 = F.relu(self.conv3(en_layer2))\n",
    "        en_layer3 = self.pool(en_layer3)\n",
    "\n",
    "        #de_layer0 = F.relu(self.t_conv0(en_layer4)) \n",
    "        de_layer1 = F.relu(self.t_conv1(en_layer3))   \n",
    "        #de_layer1 = nn.functional.interpolate(de_layer1,scale_factor = 2)\n",
    "        de_layer2= F.relu(self.t_conv2(de_layer1)) \n",
    "        de_layer3= F.relu(self.t_conv3(de_layer2))\n",
    "        de_layer4= F.relu(self.t_conv4(de_layer3))\n",
    "        de_layer5= F.relu(self.t_conv5(de_layer4))\n",
    "        #de_layer6= F.relu(self.t_conv6(de_layer5))\n",
    "        \n",
    "        #de_layerout = torch.sigmoid(self.t_conv3(de_layer2))\n",
    "        de_layerout = F.relu(self.t_convout(de_layer5))\n",
    "        #de_layerout = self.t_conv5(de_layer4)\n",
    "        \n",
    "        if self.layer_disp:\n",
    "            print(\"input\", x.shape)\n",
    "            print(\"en_layer1\",en_layer1.shape)\n",
    "            #print(\"en_layer1a\",en_layer1a.shape)\n",
    "            print(\"en_layer2\",en_layer2.shape)\n",
    "            #print(\"en_layer2a\",en_layer2a.shape)\n",
    "            print(\"en_layer3\",en_layer3.shape)\n",
    "            #print(\"en_layer3a\",en_layer3a.shape)\n",
    "            #print(\"en_layer4\",en_layer4.shape)\n",
    "            #print(\"en_layer5\",en_layer5.shape)\n",
    "            \n",
    "            #print(\"de_layer0\",de_layer0.shape)\n",
    "            print(\"de_layer1\",de_layer1.shape)\n",
    "            #print(\"de_layer1a\",de_layer1a.shape)\n",
    "            print(\"de_layer2\",de_layer2.shape)\n",
    "            print(\"de_layer3\",de_layer3.shape)\n",
    "            print(\"de_layer4\",de_layer4.shape)\n",
    "            print(\"de_layer5\",de_layer5.shape)\n",
    "            #print(\"de_layer6\",de_layer6.shape)\n",
    "            #print(\"de_layer2a\",de_layer2a.shape)\n",
    "            #print(\"de_layer3\",de_layerou.shape)\n",
    "            #print(\"de_layer3a\",de_layer3a.shape)\n",
    "            #print(\"de_layer4\",de_layer4.shape)\n",
    "            print(\"de_layerout\",de_layerout.shape)\n",
    "              \n",
    "        return de_layerout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "179b2f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([3, 128, 128])\n",
      "en_layer1 torch.Size([128, 64, 64])\n",
      "en_layer2 torch.Size([256, 16, 16])\n",
      "en_layer3 torch.Size([256, 2, 2])\n",
      "de_layer1 torch.Size([128, 4, 4])\n",
      "de_layer2 torch.Size([128, 8, 8])\n",
      "de_layer3 torch.Size([64, 16, 16])\n",
      "de_layer4 torch.Size([64, 32, 32])\n",
      "de_layer5 torch.Size([32, 64, 64])\n",
      "de_layerout torch.Size([3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "#Now to intialise the model and shit\n",
    "model = ConvAutoencoder(layer_disp = False).to(device)\n",
    "model = ConvAutoencoder(layer_disp = True).to(device)\n",
    "\n",
    "#Defining the loss function between the input and the output\n",
    "#criterion = nn.BCELoss()\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.L1Loss()\n",
    "#Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1*1e-3)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 50, gamma=0.5)\n",
    "\n",
    "#This is just for testing and seeing the outputs of each convolutional layer\n",
    "dummy = model(torch.empty(3, 128, 128).to(device))\n",
    "#from torchviz import make_dot\n",
    "#make_dot(dummy, params=dict(list(model.named_parameters()))).render(\"autoencoder\", format=\"png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc309965",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "fa9d813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from livelossplot import PlotLosses\n",
    "\n",
    "# n_epochs = 10000 #Number of epochs\n",
    "\n",
    "# liveloss = PlotLosses()\n",
    "# logs = {}\n",
    "\n",
    "\n",
    "# for epoch in range(1, n_epochs+1):\n",
    "#     # monitor training loss\n",
    "#     train_loss = 0.0\n",
    "\n",
    "#     #Training\n",
    "#     for data in train_loader:\n",
    "#         #For some reason this is needed\n",
    "#         output = data\n",
    "#         #print(output)\n",
    "#         images = output['data']\n",
    "#         labels = output['label']\n",
    "#         #images, _ = data #Don't care about the labels\n",
    "#         images = images.to(device) \n",
    "#         labels = labels.to(device) \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(images)\n",
    "#         #outputs = model(labels) #to test if the autoencoder works\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_loss += loss.item()*images.size(0)\n",
    "          \n",
    "#     train_loss = train_loss/len(train_loader)\n",
    "#     logs['loss'] = train_loss\n",
    "    \n",
    "#     #print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n",
    "#     predictions = outputs\n",
    "#     predictions = predictions.detach().cpu().numpy()\n",
    "#     imshow(predictions[0])\n",
    "#     if not(epoch%50) or epoch ==0:\n",
    "#         plt.show()\n",
    "\n",
    "#     liveloss.update(logs)\n",
    "#     liveloss.send()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db8086d",
   "metadata": {},
   "source": [
    "# Display the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "2ea27f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([27, 3, 128, 128])\n",
      "en_layer1 torch.Size([27, 128, 64, 64])\n",
      "en_layer2 torch.Size([27, 256, 16, 16])\n",
      "en_layer3 torch.Size([27, 256, 2, 2])\n",
      "de_layer1 torch.Size([27, 128, 4, 4])\n",
      "de_layer2 torch.Size([27, 128, 8, 8])\n",
      "de_layer3 torch.Size([27, 64, 16, 16])\n",
      "de_layer4 torch.Size([27, 64, 32, 32])\n",
      "de_layer5 torch.Size([27, 32, 64, 64])\n",
      "de_layerout torch.Size([27, 3, 128, 128])\n",
      "(27, 3, 128, 128)\n",
      "torch.Size([27, 3, 128, 128])\n",
      "Sensor Input\n",
      "0.2657782\n",
      "0.048152167\n",
      "0.26614583\n",
      "0.047962554\n",
      "0.2653799\n",
      "0.04808199\n",
      "0.26207107\n",
      "0.046242695\n",
      "0.26737136\n",
      "0.04871088\n",
      "0.26427698\n",
      "0.04979349\n",
      "0.26430762\n",
      "0.048154976\n",
      "0.2650429\n",
      "0.04853815\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Basically load the created class in\n",
    "test_dataset = SimData(\"test\")\n",
    "\n",
    "model.load_state_dict(torch.load(\"/home/parth/UROP_2023/deblur.pth\", map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, num_workers=0, shuffle = True)\n",
    "#Batch of test images\n",
    "with torch.no_grad():\n",
    "    dataiter = iter(test_loader)\n",
    "    #images, labels = next(dataiter)\n",
    "    output = next(dataiter)\n",
    "    images = output['data'].to(device) #these are the inputs\n",
    "    labels = output['label'].to(device)\n",
    "    #labels = output['label'].numpy() #again labels aren't needed\n",
    "\n",
    "    #Sample outputs\n",
    "    #predictions = model(labels) #test just the atoencoder ibit\n",
    "    predictions = model(images) #output of the network\n",
    "    images = images.detach().cpu().numpy() #convert the images back to numpy arrays\n",
    "    labels = labels.detach().cpu().numpy() #convert the images back to numpy arrays\n",
    "\n",
    "    #convert back to appropriate numpy array and shit\n",
    "    print(images.shape)\n",
    "    print(predictions.shape)\n",
    "    predictions = predictions.detach().cpu().numpy()\n",
    "    #predictions = predictions.view(batch_size, 3, 32, 32)\n",
    "\n",
    "\n",
    "    #Original Images\n",
    "    print(\"Sensor Input\")\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=8, sharex=True, sharey=True, figsize=(12,4))\n",
    "    for idx in np.arange(8):\n",
    "        ax = fig.add_subplot(1, 8, idx+1, xticks=[], yticks=[])\n",
    "        imshow(images[idx])\n",
    "\n",
    "        input = np.transpose(images[idx], (1, 2, 0))\n",
    "        input = np.array(255*input, dtype = 'uint8')\n",
    "        input = cv2.cvtColor(input, cv2.COLOR_RGB2BGR)\n",
    "        os.makedirs('inputs', exist_ok=True) \n",
    "        base_path = os.path.join('inputs',\"im{}.jpg\".format(idx)) #training outputs\n",
    "        cv2.imwrite(base_path,input)\n",
    "        #ax.set_title(classes[labels[idx]])\n",
    "    plt.show()\n",
    "    \n",
    "    # #Original Images\n",
    "    # print(\"Real Shape Images\")\n",
    "    # fig, axes = plt.subplots(nrows=1, ncols=5, sharex=True, sharey=True, figsize=(12,4))\n",
    "    # for idx in np.arange(5):\n",
    "    #     ax = fig.add_subplot(1, 5, idx+1, xticks=[], yticks=[])\n",
    "    #     imshow(labels[idx])\n",
    "    #     #ax.set_title(classes[labels[idx]])\n",
    "    # plt.show()\n",
    "\n",
    "    #Reconstructed Images\n",
    "    print('Reconstructed Predicted Shape Images')\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=8, sharex=True, sharey=True, figsize=(12,4))\n",
    "    for idx in np.arange(8):\n",
    "        ax = fig.add_subplot(1, 8, idx+1, xticks=[], yticks=[])\n",
    "        imshow(predictions[idx])\n",
    "\n",
    "        pred = np.transpose(predictions[idx], (1, 2, 0))\n",
    "        pred = np.array(255*pred, dtype = 'uint8')\n",
    "        pred = cv2.cvtColor(pred, cv2.COLOR_RGB2BGR)\n",
    "        os.makedirs('predictions', exist_ok=True) \n",
    "        base_path = os.path.join('predictions',\"im{}.jpg\".format(idx)) #training outputs\n",
    "       \n",
    "        cv2.imwrite(base_path,pred) \n",
    "        #ax.set_title(classes[labels[idx]])\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a564d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST WITH DATA FROM THE LARGER SET???'"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''TEST WITH DATA FROM THE LARGER SET???'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03db585b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
